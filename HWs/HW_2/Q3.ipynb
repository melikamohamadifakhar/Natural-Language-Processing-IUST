{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui0pGWAdOM-4"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXRf5daeT4NG",
        "outputId": "d22885c0-b8f6-489d-b4db-842a1009ecec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oCb3cVwzrfbb"
      },
      "outputs": [],
      "source": [
        "SOURCE_DIR = '/content/drive/MyDrive/NLP-M/Q3_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "onlO_wuSOM-8"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def delete_ex(text):\n",
        "  text = re.sub(r'\\u200c', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "# 0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t0FjQ9_ZMkve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de430496-ea1a-4603-dc18-ac49900883ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: json-lines in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from json-lines) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uf7K92-olSfe"
      },
      "outputs": [],
      "source": [
        "import json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bG5awXKo40HS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "fbc6a3c5-ee13-4f87-c9e3-524a946b59f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                   Datetime  \\\n",
              "0           0  2022-09-22 09:14:35+00:00   \n",
              "1           1  2022-10-06 01:44:55+00:00   \n",
              "2           2  2022-09-22 15:12:28+00:00   \n",
              "3           3  2022-09-22 09:35:50+00:00   \n",
              "4           4  2022-09-22 01:31:25+00:00   \n",
              "5           5  2022-09-26 21:05:14+00:00   \n",
              "6           6  2022-09-22 20:37:50+00:00   \n",
              "7           7  2022-09-28 05:27:37+00:00   \n",
              "8           8  2022-10-09 03:40:09+00:00   \n",
              "9           9  2022-09-24 21:46:20+00:00   \n",
              "\n",
              "                                                Text  \\\n",
              "0  بنشین تا شود نقش فال ما \\nنقش هم‌ فردا شدن\\n#م...   \n",
              "1  @Tanasoli_Return @dr_moosavi این گوزو رو کی گر...   \n",
              "2  @ghazaleghaffary برای ایران، برای مهسا.\\n#OpIr...   \n",
              "3  @_hidden_ocean مرگ بر دیکتاتور \\n#OpIran \\n#Ma...   \n",
              "4  نذاریم خونشون پایمال شه.‌‌.‌‌.\\n#Mahsa_Amini #...   \n",
              "5  @Nabauti88 مابهت افتخار میکنیم نبات باعث شدی ک...   \n",
              "6  @Bunnpaw برای انسانای خوشگلمون\\n\\n#مهسا_امینی ...   \n",
              "7  @neginsh فارغ از هر باوری متحد شویم.\\n#مهسا_ام...   \n",
              "8  @mansurehhossai2 اینها عجب موجودات پستی هستن🥺🥺...   \n",
              "9  @ShahinMaghsoodi کصخلا چرا ۴ تاوفحشش نمیدن؟\\n\\...   \n",
              "\n",
              "                                            PureText Language  \\\n",
              "0           بنشین تا شود نقش فال ما نقش هم‌ فردا شدن       fa   \n",
              "1  این گوزو رو کی گردن میگیره؟؟ دچار زوال عقل شده...       fa   \n",
              "2                             برای ایران، برای مهسا.       fa   \n",
              "3                                    مرگ بر دیکتاتور       fa   \n",
              "4                     نذاریم خونشون پایمال شه.‌‌.‌‌.       fa   \n",
              "5  مابهت افتخار میکنیم نبات باعث شدی کل دنیا مارو...       fa   \n",
              "6                              برای انسانای خوشگلمون       fa   \n",
              "7                        فارغ از هر باوری متحد شویم.       fa   \n",
              "8  اینها عجب موجودات پستی هستن🥺🥺🥺الهی بگردم، من خ...       fa   \n",
              "9                         کصخلا چرا ۴ تاوفحشش نمیدن؟       fa   \n",
              "\n",
              "                Sentiment        Date  \\\n",
              "0                negative  2022-09-22   \n",
              "1           very negative  2022-10-06   \n",
              "2                positive  2022-09-22   \n",
              "3           very negative  2022-09-22   \n",
              "4                negative  2022-09-22   \n",
              "5                positive  2022-09-26   \n",
              "6                positive  2022-09-22   \n",
              "7  no sentiment expressed  2022-09-28   \n",
              "8           very negative  2022-10-09   \n",
              "9           very negative  2022-09-24   \n",
              "\n",
              "                                       cleaned_tweet  \n",
              "0            بنشین تا شود نقش فال ما نقش هم فردا شدن  \n",
              "1  این گوزو رو کی گردن میگیره؟؟ دچار زوال عقل شده...  \n",
              "2                             برای ایران، برای مهسا.  \n",
              "3                                    مرگ بر دیکتاتور  \n",
              "4                         نذاریم خونشون پایمال شه...  \n",
              "5  مابهت افتخار میکنیم نبات باعث شدی کل دنیا مارو...  \n",
              "6                              برای انسانای خوشگلمون  \n",
              "7                        فارغ از هر باوری متحد شویم.  \n",
              "8  اینها عجب موجودات پستی هستن🥺🥺🥺الهی بگردم، من خ...  \n",
              "9                         کصخلا چرا ۴ تاوفحشش نمیدن؟  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2aa6292-4a9e-4a9e-8092-f822f138bc54\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Text</th>\n",
              "      <th>PureText</th>\n",
              "      <th>Language</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Date</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2022-09-22 09:14:35+00:00</td>\n",
              "      <td>بنشین تا شود نقش فال ما \\nنقش هم‌ فردا شدن\\n#م...</td>\n",
              "      <td>بنشین تا شود نقش فال ما نقش هم‌ فردا شدن</td>\n",
              "      <td>fa</td>\n",
              "      <td>negative</td>\n",
              "      <td>2022-09-22</td>\n",
              "      <td>بنشین تا شود نقش فال ما نقش هم فردا شدن</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2022-10-06 01:44:55+00:00</td>\n",
              "      <td>@Tanasoli_Return @dr_moosavi این گوزو رو کی گر...</td>\n",
              "      <td>این گوزو رو کی گردن میگیره؟؟ دچار زوال عقل شده...</td>\n",
              "      <td>fa</td>\n",
              "      <td>very negative</td>\n",
              "      <td>2022-10-06</td>\n",
              "      <td>این گوزو رو کی گردن میگیره؟؟ دچار زوال عقل شده...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2022-09-22 15:12:28+00:00</td>\n",
              "      <td>@ghazaleghaffary برای ایران، برای مهسا.\\n#OpIr...</td>\n",
              "      <td>برای ایران، برای مهسا.</td>\n",
              "      <td>fa</td>\n",
              "      <td>positive</td>\n",
              "      <td>2022-09-22</td>\n",
              "      <td>برای ایران، برای مهسا.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2022-09-22 09:35:50+00:00</td>\n",
              "      <td>@_hidden_ocean مرگ بر دیکتاتور \\n#OpIran \\n#Ma...</td>\n",
              "      <td>مرگ بر دیکتاتور</td>\n",
              "      <td>fa</td>\n",
              "      <td>very negative</td>\n",
              "      <td>2022-09-22</td>\n",
              "      <td>مرگ بر دیکتاتور</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2022-09-22 01:31:25+00:00</td>\n",
              "      <td>نذاریم خونشون پایمال شه.‌‌.‌‌.\\n#Mahsa_Amini #...</td>\n",
              "      <td>نذاریم خونشون پایمال شه.‌‌.‌‌.</td>\n",
              "      <td>fa</td>\n",
              "      <td>negative</td>\n",
              "      <td>2022-09-22</td>\n",
              "      <td>نذاریم خونشون پایمال شه...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>2022-09-26 21:05:14+00:00</td>\n",
              "      <td>@Nabauti88 مابهت افتخار میکنیم نبات باعث شدی ک...</td>\n",
              "      <td>مابهت افتخار میکنیم نبات باعث شدی کل دنیا مارو...</td>\n",
              "      <td>fa</td>\n",
              "      <td>positive</td>\n",
              "      <td>2022-09-26</td>\n",
              "      <td>مابهت افتخار میکنیم نبات باعث شدی کل دنیا مارو...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>2022-09-22 20:37:50+00:00</td>\n",
              "      <td>@Bunnpaw برای انسانای خوشگلمون\\n\\n#مهسا_امینی ...</td>\n",
              "      <td>برای انسانای خوشگلمون</td>\n",
              "      <td>fa</td>\n",
              "      <td>positive</td>\n",
              "      <td>2022-09-22</td>\n",
              "      <td>برای انسانای خوشگلمون</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>2022-09-28 05:27:37+00:00</td>\n",
              "      <td>@neginsh فارغ از هر باوری متحد شویم.\\n#مهسا_ام...</td>\n",
              "      <td>فارغ از هر باوری متحد شویم.</td>\n",
              "      <td>fa</td>\n",
              "      <td>no sentiment expressed</td>\n",
              "      <td>2022-09-28</td>\n",
              "      <td>فارغ از هر باوری متحد شویم.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>2022-10-09 03:40:09+00:00</td>\n",
              "      <td>@mansurehhossai2 اینها عجب موجودات پستی هستن🥺🥺...</td>\n",
              "      <td>اینها عجب موجودات پستی هستن🥺🥺🥺الهی بگردم، من خ...</td>\n",
              "      <td>fa</td>\n",
              "      <td>very negative</td>\n",
              "      <td>2022-10-09</td>\n",
              "      <td>اینها عجب موجودات پستی هستن🥺🥺🥺الهی بگردم، من خ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>2022-09-24 21:46:20+00:00</td>\n",
              "      <td>@ShahinMaghsoodi کصخلا چرا ۴ تاوفحشش نمیدن؟\\n\\...</td>\n",
              "      <td>کصخلا چرا ۴ تاوفحشش نمیدن؟</td>\n",
              "      <td>fa</td>\n",
              "      <td>very negative</td>\n",
              "      <td>2022-09-24</td>\n",
              "      <td>کصخلا چرا ۴ تاوفحشش نمیدن؟</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2aa6292-4a9e-4a9e-8092-f822f138bc54')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a2aa6292-4a9e-4a9e-8092-f822f138bc54 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a2aa6292-4a9e-4a9e-8092-f822f138bc54');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac26d824-04f8-4b0d-8050-0417ac8b81f7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac26d824-04f8-4b0d-8050-0417ac8b81f7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac26d824-04f8-4b0d-8050-0417ac8b81f7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20000,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5773,\n        \"min\": 0,\n        \"max\": 19999,\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          10650,\n          2041,\n          8668\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Datetime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 19486,\n        \"samples\": [\n          \"2022-09-26 09:57:06+00:00\",\n          \"2022-09-23 11:11:43+00:00\",\n          \"2022-09-23 06:01:42+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19990,\n        \"samples\": [\n          \"@min_hasti @Dina08561473 \\u0634\\u0648\\u0644 \\u0646\\u06a9\\u0631\\u062f\\u06cc\\u0645 \\u0627\\u06a9\\u062b\\u0631\\u0627 \\u0646\\u062a \\u0646\\u062f\\u0627\\u0631\\u06cc\\u0645 \\n#Mahsa_Amini\",\n          \"\\u062e\\u06cc\\u0644\\u06cc \\u062f\\u0644\\u0645 \\u0645\\u06cc\\u062e\\u0648\\u0627\\u062f \\u06cc\\u0647 \\u0633\\u0631\\u06cc \\u0627\\u0632 \\u0645\\u062f\\u06cc\\u0631\\u0627 \\u0648 \\u0646\\u0627\\u0638\\u0645\\u200c\\u0647\\u0627 \\u0648 \\u0645\\u0639\\u0644\\u0645\\u200c\\u0647\\u0627 \\u0648 \\u0627\\u0633\\u062a\\u0627\\u062f\\u200c\\u0647\\u0627 \\u0637\\u06cc \\u06cc\\u0647 \\u0627\\u062a\\u0641\\u0627\\u0642 \\u0646\\u0627\\u06af\\u0648\\u0627\\u0631 \\u062a\\u0648\\u06cc \\u0622\\u062a\\u06cc\\u0634\\u06cc \\u06a9\\u0647 \\u0628\\u0647 \\u062c\\u0648\\u0646 \\u0645\\u0627\\u0634\\u06cc\\u0646\\u0634\\u0648\\u0646 \\u0627\\u0641\\u062a\\u0627\\u062f\\u0647 \\u0632\\u0646\\u062f\\u0647 \\u0632\\u0646\\u062f\\u0647 \\u0628\\u0633\\u0648\\u0632\\u0646\\n#\\u0645\\u0647\\u0633\\u0627_\\u0627\\u0645\\u06cc\\u0646\\u06cc\\n#MahsaAmini \\n#OpIran \\n#Mahsa_Amini\\n#\\u0627\\u0639\\u062a\\u0635\\u0627\\u0628\\u0627\\u062a_\\u0633\\u0631\\u0627\\u0633\\u0631\\u06cc\\n#\\u0633\\u06cc\\u0633\\u062a\\u0627\\u0646_\\u0648_\\u0628\\u0644\\u0648\\u0686\\u0633\\u062a\\u0627\\u0646\\n#StopHazaraGenocide\",\n          \"@SorenaOfficial \\u0686\\u0647\\u0627\\u0631\\u0635\\u062f \\u0648 \\u0633\\u06cc \\u0648 \\u0646\\u0647 \\n#\\u0645\\u0647\\u0633\\u0627_\\u0627\\u0645\\u06cc\\u0646\\u06cc \\n#Mahsa_Amini \\n#OpIran\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PureText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16882,\n        \"samples\": [\n          \"\\u0645\\u0646 \\u06a9\\u0647 \\u062f\\u0648\\u0633\\u062a \\u062f\\u0627\\u0634\\u062a\\u0645\\u060c \\u0622\\u062e\\u0647 \\u0686\\u0647 \\u0641\\u06a9\\u0631\\u06cc \\u06a9\\u0631\\u062f\\u06cc \\u0627\\u06cc\\u0646 \\u0645\\u062a\\u0646\\u0648 \\u0646\\u0648\\u0634\\u062a\\u06cc\\ud83d\\ude1e\",\n          \"\\u0635\\u062f\\u0627\\u0645\\u0648\\u0646\\u0648 \\u06a9\\u0644 \\u062c\\u0647\\u0627\\u0646 \\u062f\\u0627\\u0631\\u0646 \\u0645\\u06cc\\u0634\\u0646\\u0648\\u0646 \",\n          \"\\u0686\\u0648\\u0646 \\u0627\\u06cc\\u0646\\u062c\\u0627 \\u062d\\u0633\\u0627\\u0628 \\u06a9\\u062a\\u0627\\u0628 \\u062f\\u0627\\u0631\\u0647 \\u0647\\u0645\\u0647 \\u0686\\u06cc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"fa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 90,\n        \"samples\": [\n          \"2022-11-09\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16867,\n        \"samples\": [\n          \"\\u06cc\\u0639\\u0646\\u06cc \\u0645\\u06cc\\u0634\\u0647 \\u062f\\u06cc\\u06af\\u0647 \\u0628\\u0647 \\u0645\\u0647\\u0627\\u062c\\u0631\\u062a \\u0641\\u06a9\\u0631 \\u0646\\u06a9\\u0646\\u06cc\\u0645\\u061f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 1. extract all tweets from file and save them in memory\n",
        "# 2. remove urls, hashtags and usernames. use the prepared functions\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(SOURCE_DIR, engine='python')\n",
        "\n",
        "# Function to clean a single tweet\n",
        "def clean_tweet(tweet):\n",
        "    tweet = delete_hashtag_usernames(tweet)\n",
        "    tweet = delete_url(tweet)\n",
        "    tweet = delete_ex(tweet)\n",
        "    return tweet\n",
        "\n",
        "# Clean all tweets in the dataframe\n",
        "df['cleaned_tweet'] = df['Text'].apply(clean_tweet)\n",
        "\n",
        "\n",
        "# Show first 10 rows\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7wS4bFoOM-9"
      },
      "source": [
        "# 1. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L50-MFv2OM-9"
      },
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows:\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$.\n",
        "* The cosine similarity depends on the angle between $u$ and $v$.\n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value.\n",
        "\n",
        "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>\n",
        "\n",
        "Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nJNIA8MrOM--"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "      # Compute the dot product between u and v (u.v)\n",
        "    dot_product = np.dot(u, v)\n",
        "\n",
        "    # Compute the L2 norm of u (|u|)\n",
        "    norm_u = np.linalg.norm(u)\n",
        "\n",
        "    # Compute the L2 norm of v (|v|)\n",
        "    norm_v = np.linalg.norm(v)\n",
        "\n",
        "    # Compute the cosine similarity defined by formula (1)\n",
        "    cosine_similarity = dot_product / (norm_u * norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbkVS03iOM--"
      },
      "source": [
        "## find k nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K-pnsuEfOM--"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  \"\"\"\n",
        "    implement a function to return the nearest words to an specific word based on the given dictionary\n",
        "\n",
        "    Arguments:\n",
        "        word           -- a word, string\n",
        "        embedding_dict -- dictionary that maps words to their corresponding vectors\n",
        "        k              -- the number of word that should be returned\n",
        "\n",
        "    Returns:\n",
        "        a list of size k consisting of the k most similar words to the given word\n",
        "\n",
        "    Note: use the cosine_similarity function that you have implemented to calculate the similarity between words\n",
        "    \"\"\"\n",
        "  similarities = {}\n",
        "\n",
        "  # Check if the word is in the embedding dictionary\n",
        "  if word not in embedding_dict:\n",
        "      raise ValueError(\"Word not found in the embedding dictionary.\")\n",
        "\n",
        "  # Get the embedding vector of the given word\n",
        "  word_vector = embedding_dict[word]\n",
        "\n",
        "  # Calculate the cosine similarity with every other word in the embedding dictionary\n",
        "  for other_word, other_vector in embedding_dict.items():\n",
        "      if other_word != word:  # Skip the query word itself\n",
        "          sim = cosine_similarity(word_vector, np.array(other_vector))\n",
        "          similarities[other_word] = sim\n",
        "\n",
        "  # Sort words by their similarity score in descending order\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  # Extract the top k most similar words\n",
        "  nearest_neighbors = [word for word, similarity in sorted_similarities[:k]]\n",
        "\n",
        "  return nearest_neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "# 2. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OPqc0I0yuNlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba0ba8c-6a57-4573-ac90-9c49e2f271da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['احمقهای', 'نکند', 'مطبم', 'دقیقهای', 'همين', 'رفته،', 'ازادی..', 'مملکتمون،دوری', 'مشهدی', 'کشورت']\n"
          ]
        }
      ],
      "source": [
        "# 1. find one hot encoding of each word\n",
        "# 2. find 10 nearest words from \"آزادی\"\n",
        "\n",
        "all_words = set()\n",
        "df['cleaned_tweet'].apply(lambda tweet: all_words.update(tweet.split()))\n",
        "\n",
        "# Convert the set to a list and reshape for OneHotEncoder\n",
        "unique_words_list = list(all_words)\n",
        "unique_words_array = np.array(unique_words_list).reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse=False)  # Use sparse=False to get a dense array for demonstration\n",
        "one_hot_encoded_words = encoder.fit_transform(unique_words_array)\n",
        "\n",
        "# Creating a dictionary mapping words to their one-hot encoded vectors\n",
        "word_to_one_hot = dict(zip(unique_words_list, one_hot_encoded_words))\n",
        "\n",
        "embedding_dict_one_hot = word_to_one_hot\n",
        "\n",
        "# Find 10 \"nearest\" words to \"آزادی\" based on one-hot encoding\n",
        "# This will not yield meaningful semantic results\n",
        "try:\n",
        "    nearest_neighbors = find_k_nearest_neighbors(\"آزادی\", embedding_dict_one_hot, 10)\n",
        "    print(nearest_neighbors)\n",
        "except ValueError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJR3iFAUOM--"
      },
      "source": [
        "##### Describe advantages and disadvantages of one-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tAFn4brKrB"
      },
      "source": [
        "**Advantage:** <br>\n",
        "\n",
        "*Simplicity:* One-hot encoding is straightforward to\n",
        "understand and implement.\n",
        "<br><br>\n",
        "*No Implicit Ordering:* It does not impose any order or hierarchy on the categorical values, unlike numerical encoding. This prevents the model from assuming a natural ordering between categories where none exists, which can be particularly important for nominal variables where no such ordinal relationship should influence the model's performance.\n",
        "<br><br>\n",
        "*Compatibility with Algorithms:* Many machine learning models, especially linear models, require input data to be numerical. One-hot encoding converts categorical data into a numeric format, making it possible to train models that otherwise couldn’t handle categorical data directly.\n",
        "\n",
        "**Disadvantage:** <br>\n",
        "*Dimensionality Increase:* One of the biggest downsides of one-hot encoding is the significant increase in the dataset's dimensionality.\n",
        "<br><br>\n",
        "*Sparse Matrix:* The resulting encoded data is very sparse (mostly zeros), which can be inefficient for both storage and computation.\n",
        "<br><br>\n",
        "*Loss of Information:* One-hot encoding treats each category as independent without considering the potential relationships between categories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeSYFUKw5gw"
      },
      "source": [
        "# 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m8jCoXdP-iTG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Calculate Term Frequency (TF)\n",
        "def compute_tf(text):\n",
        "    words = text.split()\n",
        "    tf = {word: words.count(word) / len(words) for word in set(words)}\n",
        "    return tf\n",
        "\n",
        "# Calculate Inverse Document Frequency (IDF)\n",
        "def compute_idf(documents):\n",
        "    N = len(documents)\n",
        "    idf = defaultdict(lambda: math.log(N))\n",
        "    for word in set(word for document in documents for word in document.split()):\n",
        "        idf[word] -= math.log(sum(word in document for document in documents))\n",
        "    return idf\n",
        "\n",
        "# Calculate TF-IDF for all documents\n",
        "def compute_tfidf(documents):\n",
        "    idf = compute_idf(documents)\n",
        "    tfidf_documents = []\n",
        "    for document in documents:\n",
        "        tf = compute_tf(document)\n",
        "        tfidf = {word: tf_score * idf[word] for word, tf_score in tf.items()}\n",
        "        tfidf_documents.append(tfidf)\n",
        "    return tfidf_documents, list(idf.keys())\n",
        "\n",
        "# Convert TF-IDF dictionaries to vectors\n",
        "def tfidf_to_vectors(tfidf_documents, feature_words):\n",
        "    vectors = []\n",
        "    for doc in tfidf_documents:\n",
        "        vector = [doc.get(word, 0) for word in feature_words]\n",
        "        vectors.append(vector)\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Cosine Similarity\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    norm_a = np.linalg.norm(vector1)\n",
        "    norm_b = np.linalg.norm(vector2)\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "# Find nearest neighbors based on cosine similarity\n",
        "def find_nearest_neighbors(vector, vectors, n_neighbors=10):\n",
        "    similarities = [cosine_similarity(vector, v) for v in vectors]\n",
        "    nearest_indices = np.argsort(similarities)[-n_neighbors-1:-1][::-1]\n",
        "    return nearest_indices\n",
        "\n",
        "# Process the documents\n",
        "def process_documents(documents):\n",
        "    tfidf_documents, feature_words = compute_tfidf(documents)\n",
        "    vectors = tfidf_to_vectors(tfidf_documents, feature_words)\n",
        "    return vectors\n",
        "\n",
        "def choose_random(documents, vectors):\n",
        "    # Choose a random document\n",
        "    random_index = np.random.randint(len(documents))\n",
        "    chosen_vector = vectors[random_index]\n",
        "\n",
        "    # Find the nearest neighbors\n",
        "    nearest_indices = find_nearest_neighbors(chosen_vector, vectors)\n",
        "\n",
        "    # Print the chosen document and its nearest neighbors\n",
        "    print(f\"Chosen Document: {documents[random_index]}\")\n",
        "    print(\"Nearest Neighbors:\")\n",
        "    for index in nearest_indices:\n",
        "        print(documents[index])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df['cleaned_tweet'].tolist()\n",
        "vectors = process_documents(documents)"
      ],
      "metadata": {
        "id": "HuQNC4UGe7l8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "choose_random(documents, vectors)"
      ],
      "metadata": {
        "id": "P4dt1DQMe7_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048f3963-3788-4522-e6f3-8ff4da8966a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Document: برای زخم هایی که هر روز تازه اند... برای دردهای مادران عزادار...\n",
            "Nearest Neighbors:\n",
            "برای تمام دردهای فرو خورده\n",
            "برای تمام مادران ایران زمین\n",
            "برای همه مادران و دختران ایران زمین\n",
            "برای اشک های مادران داغ دار\n",
            "برای تمام مادران داغدار ایران\n",
            "براي مادران داغدار\n",
            "برای مادران چشم انتظار ….\n",
            "برای بغض مادران چشم به راه آبان\n",
            "برای تمام مادران دادخواه... :)\n",
            "تازه وصل شدن\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del vectors"
      ],
      "metadata": {
        "id": "mE0nIHmwVPYZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del documents"
      ],
      "metadata": {
        "id": "B1scwUEFVfQP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w46N6CRKOM-_"
      },
      "source": [
        "##### Describe advantages and disadvantages of TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHvpTAZu7ZU"
      },
      "source": [
        "**Advatages:** <br>\n",
        "*Relevance Measurement:* TF-IDF provides a simple yet effective way to quantify the relevance of words within a document relative to a collection of documents. This helps in identifying and prioritizing the most significant words in texts.\n",
        "<br><br>\n",
        "*Reduction of Noise:* By considering inverse document frequency, TF-IDF naturally filters out common words that appear in many documents (such as \"the\", \"is\", and \"and\"), which are typically less informative.\n",
        "<br><br>\n",
        "*Easy to Compute and Understand:* The calculation of TF-IDF is straightforward, making it easy to implement and scale.\n",
        "<br><br>\n",
        "\n",
        "**Disadvantages:** <br>\n",
        "*Lack of Context and Order:* TF-IDF treats each document as a bag of words, meaning it doesn't capture the order of words, context, or semantics of the terms. This can lead to a loss of valuable information.\n",
        "<br><br>\n",
        "*Not Suitable for Understanding Word Relationships:* Since TF-IDF focuses on individual word importance without considering the relationship between words, it cannot capture synonyms or polysemy effectively.\n",
        "<br><br>\n",
        "*Ignores Document Length:* TF-IDF does not normalize for document length. A longer document could have higher TF-IDF scores simply due to having more words, even if the relevance of specific terms is not higher than in shorter documents.\n",
        "<br><br>\n",
        "*High-Dimensional Sparse Vectors:* Similar to one-hot encoding, TF-IDF representation can lead to very high-dimensional feature spaces, especially with large corpora containing many unique words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM6vtm2zqJs"
      },
      "source": [
        "# 4. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TCnxqaVY2zCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c3f4fd-b5d9-4904-a127-fb68b9155d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 10 nearest words to 'آزادی' are:\n",
            "ازادی: 0.9979506134986877\n",
            "زن،: 0.9945236444473267\n",
            "آزادی،: 0.9939445853233337\n",
            "زندگی،: 0.9937655925750732\n",
            "آزادی.: 0.9921770691871643\n",
            "ایرانم: 0.9921579957008362\n",
            "وطنم: 0.991191565990448\n",
            "زن: 0.989142894744873\n",
            "زندگی: 0.989051342010498\n",
            "میهن: 0.9887256026268005\n"
          ]
        }
      ],
      "source": [
        "# 1. train a word2vec model base on all tweets\n",
        "# 2. find 10 nearest words from \"آزادی\"\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize cleaned tweets:\n",
        "sentences = [tweet.split() for tweet in df['cleaned_tweet'].tolist()]\n",
        "\n",
        "# 1. Train a Word2Vec model based on all tweets\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# 2. Find 10 nearest words from \"آزادی\"\n",
        "word = \"آزادی\"\n",
        "if word in model.wv.key_to_index:\n",
        "    nearest_words = model.wv.most_similar(word, topn=10)\n",
        "    print(f\"The 10 nearest words to '{word}' are:\")\n",
        "    for similar_word, similarity in nearest_words:\n",
        "        print(f\"{similar_word}: {similarity}\")\n",
        "else:\n",
        "    print(f\"The word '{word}' is not in the model's vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIfQLsyEOM-_"
      },
      "source": [
        "##### Describe advantages and disadvantages of Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv39LXf0wkjd"
      },
      "source": [
        "**Advantages:** <br>\n",
        "*Semantic Similarity:* One of the biggest strengths of Word2Vec is its ability to capture semantic similarity between words. Words that are used in similar contexts are positioned closely in the vector space, which allows for capturing nuances in meaning.\n",
        "<br><br>\n",
        "*Dimensionality Reduction:* Compared to one-hot encoding, Word2Vec significantly reduces the dimensionality of the feature space while retaining semantic information.\n",
        "<br><br>\n",
        "*Efficiency in Training:* Despite its complexity, Word2Vec is relatively efficient to train compared to deeper, more complex models, especially when using the negative sampling training method.\n",
        "<br><br>\n",
        "\n",
        "**Disadvantages:** <br>\n",
        "*Lack of Word Sense Disambiguation:* Word2Vec assigns a single vector to each word, which means it cannot distinguish between different meanings of a word used in different contexts.\n",
        "<br><br>\n",
        "*Requirement for Large Training Corpora:* To capture rich semantic relationships, Word2Vec models require large amounts of training data.\n",
        "<br><br>\n",
        "*Static Word Embeddings:* Once trained, the embeddings are static and do not change. This means that the model cannot adapt to new words or phrases that weren't in the training data, nor can it evolve with language use over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "# 5. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zgVAEQhyOPxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bae4e13-51e6-4afb-9f51-a13e96b98ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GfKEqNml6eEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04acb7f2-28d9-4304-db38-b880b1199d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load model and tokenizer\n",
        "from transformers import BertModel, BertTokenizer, AdamW, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.tweets = df['Text'].tolist()  # Convert the 'Text' column to a list\n",
        "        self.labels = df['Sentiment'].map({'negative': 0, 'very negative': 1, 'positive': 2, 'no sentiment expressed': 3, 'very positive': 4, 'mixed': 5}).tolist()  # Convert the 'Sentiment' column to a list\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "dataset = TweetDataset(df, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "-yUmC616V2Bu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhV26zuIWJG9",
        "outputId": "073ec693-5c7b-4226-ccb4-4b4cde3a2974"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel, BertTokenizer, AdamW, BertForSequenceClassification\n",
        "\n",
        "# Assume model and dataloader have been defined earlier in the code.\n",
        "# Also, 'device' is assumed to be defined (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Initialize the optimizer with the model's parameters and a learning rate of 1e-5\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Set the model to training mode. This is necessary because some modules like Dropout\n",
        "# behave differently during training\n",
        "model.train()\n",
        "\n",
        "# Loop through each batch in the DataLoader. tqdm is used to display progress.\n",
        "for batch in tqdm(dataloader):\n",
        "    # Move batch data to the same device as the model to avoid CPU-GPU data transfer issues\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    # Pass the batch through the model. This step computes the forward pass.\n",
        "    # **batch unpacks the dictionary directly into the input arguments of the model.\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    # Extract labels from the batch for comparison with model outputs to compute loss\n",
        "    labels = batch['labels']\n",
        "\n",
        "    # Initialize the loss function. CrossEntropyLoss is commonly used for classification tasks.\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Compute the loss by comparing the model outputs with the true labels.\n",
        "    # 'logits' are the raw, unnormalized scores output by the last layer of the model.\n",
        "    loss = loss_function(outputs.logits, labels)\n",
        "\n",
        "    # Perform backpropagation starting from the loss to compute gradients for each parameter\n",
        "    loss.backward()\n",
        "\n",
        "    # Update model parameters based on gradients computed\n",
        "    optimizer.step()\n",
        "\n",
        "    # Clear the gradients to prevent them from being accumulated\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X71XvTY6WKZb",
        "outputId": "dda24b4d-22d2-4746-e7ac-24253bf3af03"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 1250/1250 [07:25<00:00,  2.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. find 10 nearest words from \"آزادی\"\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Get the embedding of 'آزادی'\n",
        "input_ids = tokenizer.encode('آزادی', return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    azadi_embedding = model(input_ids)[0][0, :].cpu().numpy()  # Removed one indexing dimension\n",
        "\n",
        "# Calculate the cosine similarity with all other words\n",
        "similarities = []\n",
        "for word in tqdm(tokenizer.get_vocab()):\n",
        "    input_ids = tokenizer.encode(word, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        word_embedding = model(input_ids)[0][0, :].cpu().numpy()  # Removed one indexing dimension\n",
        "    similarity = 1 - cosine(azadi_embedding, word_embedding)\n",
        "    similarities.append((word, similarity))\n",
        "\n",
        "# Sort by similarity and get the top 10 words\n",
        "similar_words = sorted(similarities, key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "similar_words"
      ],
      "metadata": {
        "id": "iMwt19OtWV3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe66cdd6-6eee-42f1-a8a6-5a15bd2925d4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [19:28<00:00, 85.55it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ویديوهای', 0.9990941882133484),\n",
              " ('بازدیدکنندگان', 0.9984754323959351),\n",
              " ('vey', 0.9982344508171082),\n",
              " ('جوزه', 0.99754798412323),\n",
              " ('اشتغالزا', 0.9975302815437317),\n",
              " ('استقلالطلبی', 0.997490406036377),\n",
              " ('تفریحات', 0.9972715973854065),\n",
              " ('۰۳۱', 0.9972620010375977),\n",
              " ('مجله', 0.9971975684165955),\n",
              " ('موسسهی', 0.997179388999939)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVu82La4OM-_"
      },
      "source": [
        "\n",
        "### Advantages\n",
        "\n",
        "1. **Context-Awareness**: Unlike static embeddings (e.g., Word2Vec, GloVe), contextualized embeddings take the entire sentence context into account. This means the representation of a word can change based on its surrounding words, allowing for a more nuanced understanding of language.\n",
        "\n",
        "2. **Handling Polysemy**: They excel at capturing the meanings of polysemous words (words with multiple meanings) depending on their usage in a sentence, which is a significant limitation of non-contextual embeddings.\n",
        "\n",
        "3. **Improved Model Performance**: Contextualized embeddings have been shown to significantly improve the performance of NLP models on a wide range of tasks, including but not limited to, sentiment analysis, question answering, and named entity recognition.\n",
        "\n",
        "4. **Transfer Learning and Few-shot Learning**: Models pretrained with contextualized embeddings on large corpora can be fine-tuned with relatively small datasets to achieve high performance, making NLP applications more accessible.\n",
        "\n",
        "5. **Deeper Linguistic Understanding**: These embeddings can capture deeper syntactic and semantic information, such as the role of a word in a sentence or relationships between words, leading to models that better understand the nuances of language.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "1. **Computational Complexity**: Generating contextualized embeddings generally requires more computational resources than static embeddings, both in terms of memory and processing power. This can be a limiting factor for deployment in resource-constrained environments.\n",
        "\n",
        "2. **Training Time**: Pretraining models to generate contextualized embeddings (e.g., BERT, GPT) involves large datasets and extensive computational resources, making the process time-consuming.\n",
        "\n",
        "3. **Model Size**: Models capable of generating high-quality contextualized embeddings are often large, with hundreds of millions (or even billions) of parameters. This can make them challenging to deploy on mobile devices or in environments with strict latency requirements.\n",
        "\n",
        "4. **Fine-tuning Challenges**: While transfer learning is a significant advantage, it also requires expertise to fine-tune these models effectively for specific tasks. Improper fine-tuning can lead to suboptimal performance or overfitting.\n",
        "\n",
        "5. **Interpretability**: The complexity of models producing contextualized embeddings can make it difficult to understand how decisions are made, posing challenges for interpretability and debugging."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}