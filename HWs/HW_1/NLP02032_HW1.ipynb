{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1: Probabilistic N-Gram Language Model(50 points)"
      ],
      "metadata": {
        "id": "a4NQTign_k_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "The objective of this question is to implement and experiment with an N-Gram language model using the Reuters dataset. The task involves building a probabilistic N-Gram model and creating a text generator based on the trained model with customizable parameters.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "\n",
        "**1.Text Preprocessing (5 points):**\n",
        "*   Implement the preprocess_text function to perform necessary text preprocessing. You may use NLTK or other relevant libraries for this task. (Already provided, no modification needed)\n",
        "\n",
        "\n",
        "**2.Build Probabilistic N-Gram Model (15 points):**\n",
        "\n",
        "*   Implement the build_probabilistic_ngram_model function to construct a probabilistic N-Gram model from the Reuters dataset.\n",
        "\n",
        "\n",
        "**3.Generate Text with Customizable Parameters (15 points):**\n",
        "\n",
        "*   Implement the generate_text function to generate text given a seed text and the probabilistic N-Gram model.\n",
        "*   The function should have parameters for probability_threshold and min_length to customize the generation process.\n",
        "*   Ensure that the generation stops when either the specified min_length is reached or the probabilities fall below probability_threshold.\n",
        "\n",
        "\n",
        "**4.Experimentation and Parameter Tuning (5 points):**\n",
        "\n",
        "*   Use Google Colab to experiment with different values of n_value, probability_threshold, and min_length.\n",
        "Find the optimal parameters that result in coherent and meaningful generated text.\n",
        "*   Provide a detailed analysis of the impact of changing each parameter on the generated text's quality.\n",
        "*   Discuss any challenges faced during parameter tuning and propose potential improvements.\n",
        "\n",
        "\n",
        "**5.Results and Conclusion (10 points):**\n",
        "\n",
        "*   Summarize your findings and present the optimal parameter values for n_value, probability_threshold, and min_length.\n",
        "*   Discuss the trade-offs and considerations when selecting these parameters.\n",
        "*   Conclude with insights gained from the experimentation."
      ],
      "metadata": {
        "id": "zDKtnG-HAH1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk import ngrams\n",
        "import random\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download the Reuters dataset if not already downloaded\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "3NWXJy-T-Vd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047d9bce-e07e-4913-f474-9b677f5d96cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9IHxAbU0N80"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Remove leading and trailing white space\n",
        "    text = text.strip()\n",
        "\n",
        "    # Replace multiple white space characters with a single space\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "    # Tokenizing\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to build a probabilistic n-gram model\n",
        "def build_probabilistic_ngram_model(corpus, n):\n",
        "    # Initialize an empty dictionary\n",
        "    model = {}\n",
        "\n",
        "    # Iterate over each piece of text in the corpus\n",
        "    for text in corpus:\n",
        "        # Generate all n-grams from the current text\n",
        "        for n_gram in ngrams(text, n):\n",
        "            # Extract the first n-1 words\n",
        "            prefix = n_gram[:-1]\n",
        "            # Extract the nth word\n",
        "            next_word = n_gram[-1]\n",
        "\n",
        "            # Check if the prefix is already in the model\n",
        "            if prefix not in model:\n",
        "                model[prefix] = {}  # Initialize an empty dictionary for this prefix\n",
        "\n",
        "            # Check if the next word is already associated with the prefix\n",
        "            if next_word not in model[prefix]:\n",
        "                model[prefix][next_word] = 0  # Initialize the count for this word\n",
        "\n",
        "            model[prefix][next_word] += 1  # Increment the count\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    for prefix in model.keys():\n",
        "        total_count = sum(model[prefix].values())\n",
        "        for next_word in model[prefix].keys():\n",
        "            model[prefix][next_word] /= total_count\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to generate text using the probabilistic n-gram model with stop criteria\n",
        "def generate_text(model, seed_text, n, probability_threshold=0.1, min_length=10):\n",
        "\n",
        "    # Preprocess the seed text to tokenize it\n",
        "    tokenized_seed = preprocess_text(seed_text)\n",
        "    # Initialize the current context tokens\n",
        "    current_tokens = tokenized_seed[-(n-1):] if len(tokenized_seed) > n-1 else tokenized_seed\n",
        "    # Initialize the generated text\n",
        "    generated_text = tokenized_seed[:]\n",
        "\n",
        "    # Loop to generate text until stop criteria are met\n",
        "    while True:\n",
        "\n",
        "        if tuple(current_tokens) not in model:\n",
        "            # If the specific n-gram context isn't found, you can opt to break or choose a different strategy\n",
        "            # For simplicity, let's break here; you might want to implement a more sophisticated fallback\n",
        "            break\n",
        "        # Retrieve possible next words from the model based on the current context\n",
        "        possible_next_words = model[tuple(current_tokens)]\n",
        "\n",
        "        # Proceed only if there are possible next words\n",
        "        if not possible_next_words:\n",
        "            break\n",
        "\n",
        "        # Filter next words based on the probability threshold\n",
        "        filtered_words = [word for word in possible_next_words if possible_next_words[word] >= probability_threshold]\n",
        "\n",
        "        # Stop if no words meet the threshold criteria\n",
        "        if not filtered_words:\n",
        "            break\n",
        "\n",
        "        # Randomly select one of the filtered words\n",
        "        next_word = random.choice(filtered_words)\n",
        "        generated_text.append(next_word)\n",
        "\n",
        "        # Check if the generated text meets the minimum length requirement\n",
        "        if len(generated_text) - len(tokenized_seed) >= min_length:\n",
        "            break\n",
        "        current_tokens = generated_text[-(n-1):]\n",
        "\n",
        "    return ' '.join(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Reuters dataset\n",
        "corpus = [reuters.raw(file_id) for file_id in reuters.fileids()]\n",
        "\n",
        "# Preprocess the entire corpus\n",
        "preprocessed_corpus = [preprocess_text(text) for text in corpus]\n",
        "\n",
        "# Choose an n for the n-gram model\n",
        "n_value = 2  # You may change this value\n",
        "\n",
        "# Build the probabilistic n-gram model\n",
        "probabilistic_ngram_model = build_probabilistic_ngram_model(preprocessed_corpus, n_value)"
      ],
      "metadata": {
        "id": "eVVMe_s59Ngd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, seed_text, n_value, probability_threshold=0.02, min_length=5)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ],
      "metadata": {
        "id": "n-4WP7IC9Q7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd705f2d-c90d-47ec-98e2-c80f6fbc78d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: inflation is subject to be made by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of values for each parameter we want to test\n",
        "n_values = [2, 3, 4]\n",
        "threshold_values = [0.01, 0.02, 0.03]\n",
        "length_values = [5, 10, 15]\n",
        "seed_text = \"Inflation is going to\"\n",
        "\n",
        "for n in n_values:\n",
        "    probabilistic_ngram_model = build_probabilistic_ngram_model(preprocessed_corpus, n)\n",
        "    for threshold in threshold_values:\n",
        "        for length in length_values:\n",
        "            generated_text = generate_text(probabilistic_ngram_model, seed_text, n, threshold, length)\n",
        "            print(f\"n={n}, threshold={threshold}, length={length} -> {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKZ8MaDEiLQ5",
        "outputId": "e2c5081f-20dc-4356-9b5c-de8d05a91209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n=2, threshold=0.01, length=5 -> inflation is going to buy more to buy more\n",
            "n=2, threshold=0.01, length=10 -> inflation is going to sell some analysts say that it has acquired for a\n",
            "n=2, threshold=0.01, length=15 -> inflation is going to be in 1987 the first quarter but that if a share capital expenditure within three\n",
            "n=2, threshold=0.02, length=5 -> inflation is going to the company also be the\n",
            "n=2, threshold=0.02, length=10 -> inflation is going to a share in january 1987 from discontinued operations of the\n",
            "n=2, threshold=0.02, length=15 -> inflation is going to a share vs profit of its board and the company is expected the us agriculture\n",
            "n=2, threshold=0.03, length=5 -> inflation is going to be a share for the\n",
            "n=2, threshold=0.03, length=10 -> inflation is going to the company said its board chairman and the company said\n",
            "n=2, threshold=0.03, length=15 -> inflation is going to be a share for a year ago and the company said its board and the\n",
            "n=3, threshold=0.01, length=5 -> inflation is going to raise exports outside the range\n",
            "n=3, threshold=0.01, length=10 -> inflation is going to follow growth plans already in 1987 will see higher wheat\n",
            "n=3, threshold=0.01, length=15 -> inflation is going to buy a half yen to around 16 mln shareholders other income 1684 mln vs loss\n",
            "n=3, threshold=0.02, length=5 -> inflation is going to take effect may 14 domtar\n",
            "n=3, threshold=0.02, length=10 -> inflation is going to happen he said in an exchange official told kuna it\n",
            "n=3, threshold=0.02, length=15 -> inflation is going to have bought two of canadian wheat board bwb expects the fed would let the turkish\n",
            "n=3, threshold=0.03, length=5 -> inflation is going to take action on legislation to\n",
            "n=3, threshold=0.03, length=10 -> inflation is going to take over the weekend to be a\n",
            "n=3, threshold=0.03, length=15 -> inflation is going to go up as employees return and air force action it also has the ability to\n",
            "n=4, threshold=0.01, length=5 -> inflation is going to look bad and some good\n",
            "n=4, threshold=0.01, length=10 -> inflation is going to happen he said usair stock was trading at only 34\n",
            "n=4, threshold=0.01, length=15 -> inflation is going to happen i dont know its getting a little confusing said james carroll analyst at painewebber\n",
            "n=4, threshold=0.02, length=5 -> inflation is going to stop the issue of new\n",
            "n=4, threshold=0.02, length=10 -> inflation is going to cede national sovereignty we certainly seem to be picking up\n",
            "n=4, threshold=0.02, length=15 -> inflation is going to buy your product if they can get more well take it\n",
            "n=4, threshold=0.03, length=5 -> inflation is going to be news about the suit\n",
            "n=4, threshold=0.03, length=10 -> inflation is going to be allowed to press for substantial wage increases a spokesman\n",
            "n=4, threshold=0.03, length=15 -> inflation is going to be subverted by government he added william martin deputy energy secretary said the reagan administration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "***Analysis of Parameter Impact:***\n",
        "\n",
        "* **n_value**: When n is higher, the model considers a longer history of preceding words to predict the next word in the sequence.\n",
        "This increased context often leads to more coherent text because the model captures more nuances and dependencies in the language.\n",
        "While higher n values tend to produce more coherent text, they may result in less diverse output.\n",
        "With longer n-grams, the model becomes more specific and relies heavily on exact matches in the training data.\n",
        "This specificity can limit the variety of possible next words, leading to repetitive or predictable text.\n",
        "For instance, if the model encounters a rare phrase that appears only a few times in the training data, it might struggle to generalize beyond those specific instances.\n",
        "So Selecting the appropriate n value involves balancing the **trade-off** between coherence and diversity.\n",
        "\n",
        "* **probability_threshold**:\n",
        "Higher thresholds prioritize words that are highly probable given the context, resulting in more relevant text that closely aligns with the expected content.\n",
        "This ensures that the generated text stays on topic and maintains relevance to the seed text or the overall theme of the corpus.\n",
        "While setting a higher threshold can improve coherence and relevance, it may also reduce diversity by limiting the range of possible next words.\n",
        "So again there is a **trade-off** between coherence and diversity.\n",
        "\n",
        "* **min_length**: Longer minimum lengths ensured more meaningful text but sometimes limited diversity, especially with smaller corpora.\n",
        "So again there is a **trade-off** between coherence and diversity =)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ilcl_HG8vkA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2: Sentiment Analysis with Naive Bayes Classifier(50 Points)"
      ],
      "metadata": {
        "id": "dZ3XzDx7JUNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "You are tasked with implementing a Naive Bayes classifier for sentiment analysis. The provided code is incomplete, and your goal is to complete the missing parts. Additionally, you should train the classifier on a small dataset and analyze its performance.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1.**Complete the Code (35 points)**: Fill in the missing parts in the provided Python code for the Naive Bayes classifier. Pay special attention to the `extract_features` function.\n",
        "\n",
        "2.**Train and Test**: Train the Naive Bayes classifier on the training data and test it on a separate test set. Evaluate the accuracy of the classifier.\n",
        "\n",
        "3.**Analysis (15 points)**: Discuss the results. Identify any misclassifications and try to understand why the classifier may fail in those cases. Provide examples of sentences that were not predicted correctly and explain possible reasons.\n"
      ],
      "metadata": {
        "id": "NMuVkjW2XfAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "M68XJubdKeDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0360d152-f97d-4c96-fd97-ce37ac243dbd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(tokens):\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Perform stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "KSLo4_JoUcax"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "        self.class_probs = defaultdict(float)\n",
        "        self.feature_probs = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def train(self, training_data):\n",
        "        # Dictionary to count occurrences of each class\n",
        "        class_counts = defaultdict(int)\n",
        "\n",
        "        # Dictionary to count occurrences of each feature for each class\n",
        "        feature_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        # Count how often each class and feature occur together\n",
        "        for tokens, cls in training_data:\n",
        "            features = get_features(tokens)\n",
        "            class_counts[cls] += 1\n",
        "            for feature in features:\n",
        "                feature_counts[cls][feature] += 1\n",
        "\n",
        "        # Calculate the prior probabilities of each class and the likelihood of each feature given a class\n",
        "        total_examples = sum(class_counts.values())\n",
        "        for cls in self.classes:\n",
        "            self.class_probs[cls] = class_counts[cls] / total_examples\n",
        "            total_features = sum(feature_counts[cls].values())\n",
        "            for feature in feature_counts[cls]:\n",
        "                # Apply Laplace smoothing to prevent zero probabilities\n",
        "                self.feature_probs[cls][feature] = (feature_counts[cls][feature] + 1) / (total_features + len(feature_counts[cls]))\n",
        "\n",
        "    def classify(self, features):\n",
        "        # Initialize maximum probability to negative infinity\n",
        "        max_prob = float('-inf')\n",
        "        # Initialize best class to None\n",
        "        best_class = None\n",
        "        for cls in self.classes:\n",
        "            # Compute the log probability to avoid underflow\n",
        "            log_prob = math.log(self.class_probs[cls]) # Prior probability of the class\n",
        "            for feature in features:\n",
        "                # Likelihood of the feature given the class\n",
        "                log_prob += math.log(self.feature_probs[cls].get(feature, 1/len(self.feature_probs[cls])))\n",
        "            if log_prob > max_prob:\n",
        "                max_prob = log_prob\n",
        "                best_class = cls\n",
        "        return best_class"
      ],
      "metadata": {
        "id": "m2Whvjy_Jq8n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Load the movie reviews dataset from NLTK\n",
        "data = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "# Shuffle the dataset for randomness\n",
        "random.shuffle(data)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(data) * split_ratio)\n",
        "train_set = data[:split_index]\n",
        "test_set = data[split_index:]\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "classes = set(sentiment for _, sentiment in train_set)\n",
        "classifier = NaiveBayesClassifier(classes)\n",
        "classifier.train(train_set)\n",
        "\n",
        "misclassified_samples = []\n",
        "def calculate_accuracy(dataset, dataset_type):\n",
        "    # Test the classifier on the testing set\n",
        "    correct_predictions = 0\n",
        "    for example in dataset:\n",
        "        tokens, true_sentiment = example\n",
        "        features = get_features(tokens)\n",
        "        predicted_sentiment = classifier.classify(features)\n",
        "        if predicted_sentiment == true_sentiment:\n",
        "            correct_predictions += 1\n",
        "        else:\n",
        "          # Store misclassified samples\n",
        "          new_data = {}\n",
        "          new_data[\"token\"] = features\n",
        "          new_data[\"predicted\"] = predicted_sentiment\n",
        "          new_data[\"label\"] = true_sentiment\n",
        "          misclassified_samples.append(new_data)\n",
        "\n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    print(f\"{dataset_type} Accuracy: {accuracy}\")\n",
        "    return misclassified_samples\n",
        "\n",
        "calculate_accuracy(train_set, 'Train')\n",
        "misclassified = calculate_accuracy(test_set, 'Test')\n",
        "print(\"\\n\")\n",
        "\n",
        "# Choose 10 random elements from the list\n",
        "random_set = random.sample(misclassified, k=10)\n",
        "\n",
        "for i in random_set:\n",
        "  print(f'tokens: {i[\"token\"]}')\n",
        "  print(f'predicted label: {i[\"predicted\"]}, true label: {i[\"label\"]}')\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "j2jeyI6nKooE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ae6c41-1ac7-4278-9e0f-02ddc6022eb8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.74875\n",
            "Test Accuracy: 0.735\n",
            "\n",
            "\n",
            "tokens: ['take', 'two', 'old', 'die', 'men', 'lifetim', 'regret', 'hous', 'full', 'sin', 'thoroughli', 'despic', 'man', 'enough', 'lie', 'insecur', 'charact', 'defect', 'keep', 'team', 'psychiatrist', 'gain', 'employ', 'add', 'inexplic', 'meteorolog', 'amphibian', 'base', 'phenomenon', 'sum', 'magnolia', 'newest', 'film', 'paul', 'thoma', 'anderson', 'boogi', 'night', 'movi', 'tell', 'multipl', 'stori', 'weav', 'togeth', 'overlap', 'cours', 'three', 'hour', 'run', 'time', 'would', 'stori', 'worth', 'tell', 'earl', 'partridg', 'jason', 'robard', 'thousand', 'acr', 'die', 'cancer', 'bedridden', 'much', 'pain', 'obviou', 'time', 'grow', 'short', 'much', 'younger', 'wife', 'play', 'juliann', 'moor', 'ideal', 'husband', 'surpris', 'find', 'struggl', 'impend', 'death', 'marri', 'money', 'discov', 'actual', 'fallen', 'love', 'old', 'guy', 'regret', 'cheat', 'lie', 'earl', 'regret', 'cheat', 'first', 'wife', 'estrang', 'son', 'tom', 'cruis', 'eye', 'wide', 'shut', 'misogynist', 'self', 'help', 'guru', 'teach', 'men', 'seduc', 'destroy', 'sexual', 'vulgar', 'perspect', 'male', 'femal', 'relationship', 'top', 'utterli', 'unbeliev', 'meanwhil', 'jimmi', 'gator', 'philip', 'baker', 'hall', 'insid', 'also', 'die', 'cancer', 'physic', 'incapacit', 'earl', 'jimmi', 'still', 'abl', 'perform', 'duti', 'lovabl', 'host', 'long', 'run', 'quiz', 'show', 'pit', 'adult', 'children', 'current', 'whiz', 'quiz', 'kid', 'stanley', 'spector', 'jeremi', 'blackman', 'film', 'debut', 'tire', 'pressur', 'perform', 'will', 'call', 'quit', 'hand', 'quiz', 'kid', 'donni', 'smith', 'william', 'maci', 'mysteri', 'men', 'would', 'like', 'noth', 'return', 'spotlight', 'somebodi', 'pathet', 'life', 'sour', 'longer', 'capit', 'brief', '15', 'minut', 'fame', 'stanley', 'eager', 'relinquish', 'jimmi', 'famili', 'crisi', 'drug', 'addict', 'daughter', 'melora', 'walter', 'boogi', 'night', 'refus', 'anyth', 'reason', 'disclos', 'us', 'end', 'film', 'grab', 'one', 'last', 'attempt', 'happi', 'reach', 'softheart', 'cop', 'john', 'c', 'reilli', 'never', 'kiss', 'even', 'tri', 'push', 'away', 'deem', 'worthi', 'affect', 'dysfunct', 'group', 'carri', 'seem', 'intermin', 'two', 'third', 'movi', 'get', 'wors', 'liter', 'rain', 'frog', 'ye', 'frog', 'assum', 'intend', 'deu', 'ex', 'machina', 'devic', 'ineffect', 'one', 'seem', 'faze', 'charact', 'much', 'oh', 'may', 'step', 'gingerli', 'around', 'splatter', 'frog', 'corps', 'litter', 'street', 'otherwis', 'frog', 'shower', 'seem', 'chang', 'behavior', 'pattern', 'live', 'die', 'simpli', 'much', 'go', 'movi', 'distast', 'watch', 'mr', 'anderson', 'obscur', 'film', 'incorpor', 'loud', 'intrus', 'sound', 'track', 'often', 'drown', 'dialogu', 'charact', 'appar', 'rap', 'signific', 'clue', 'plot', 'develop', 'complet', 'unintelligbl', 'heavi', 'hand', 'segment', 'histor', 'occur', 'contain', 'iron', 'twist', 'set', 'absolut', 'noth', 'one', 'recur', 'theme', 'found', 'line', 'donni', 'quot', 'may', 'done', 'past', 'past', 'done', 'us', 'absolut', 'lie', 'god', 'forgiv', 'busi', 'fact', 'humbl', 'ask', 'forgiv', 'repent', 'chang', 'offend', 'mindset', 'god', 'word', 'say', 'forgiv', 'also', 'forget', 'even', 'blotteth', 'thi', 'transgress', 'mine', 'sake', 'rememb', 'thi', 'sin', 'isaiah', '43', '25', 'kjv', 'spiritu', 'one', 'keep', 'bring', 'unright', 'past', 'one', 'want', 'keep', 'us', 'state', 'condemn', 'let', 'next', 'time', 'spiritu', 'adversari', 'remind', 'past', 'take', 'great', 'pleasur', 'remind', 'futur', 'hate']\n",
            "predicted label: pos, true label: neg\n",
            "\n",
            "\n",
            "tokens: ['deserv', 'recognit', 'make', 'rel', 'youth', 'critic', 'feel', 'extrem', 'old', 'crotchety20', 'capsul', 'review', 'feel', 'good', 'famili', 'entertain', 'morph', '90', 'hour', 'half', 'commerci', 'disguis', 'unnecessari', 'remak', 'defin', 'imag', 'grown', 'man', 'launch', 'volum', 'green', 'protoplasm', 'goo', 'ass', 'rocketman', 'georg', 'jungl', 'disney', 'recent', 'eclips', 'longtim', 'champion', 'troma', 'studio', 'like', 'includ', 'fart', 'joke', 'film', 'absent', 'mind', 'professor', 'invent', 'titular', 'comput', 'gener', 'goop', 'listless', 'robin', 'william', 'manag', 'difficult', 'task', 'make', 'origin', 'lead', 'fred', 'macmurray', 'seem', 'sprightli', 'thing', 'made', 'film', 'borderlin', 'toler', 'newli', 'found', 'firm', 'belief', 'writer', 'produc', 'john', 'hugh', 'go', 'spend', 'etern', 'afterlif', 'conk', 'noggin', 'differ', 'blunt', 'instrument', 'use', 'comed', 'effect', 'film', 'like', 'odiou', 'home', 'alon', 'seri', 'take', 'kid', 'see', 'boogi', 'night', 'instead']\n",
            "predicted label: pos, true label: neg\n",
            "\n",
            "\n",
            "tokens: ['follow', 'review', 'contain', 'spoiler', 'way', '---', 'rapist', 'matt', 'frewer', 'respond', 'supergirl', 'helen', 'slater', 'queri', 'attack', 'exampl', 'mind', 'numbingli', 'bad', 'dialogu', 'supergirl', 'admit', 'love', 'superman', 'iii', 'know', 'hate', 'amongst', 'superman', 'faith', 'dismiss', 'noth', 'vehicl', 'richard', 'pryor', 'still', 'think', 'worthi', 'addit', 'man', 'steel', 'franchis', 'supergirl', 'spin', 'film', 'sort', 'seri', 'produc', 'alexand', 'ilya', 'salkind', 'even', 'rival', 'superman', 'iii', 'term', 'qualiti', 'let', 'alon', 'superman', 'superman', 'ii', 'supergirl', 'lost', 'open', 'scene', 'went', 'steadili', 'downhil', 'rest', 'two', 'hour', 'plu', 'run', 'time', 'film', 'begin', 'see', 'resid', 'argo', 'citi', 'commun', 'like', 'place', 'consist', 'refuge', 'krypton', 'resid', 'inner', 'space', 'go', 'daili', 'live', 'ok', 'exil', 'krypton', 'blew', 'right', 'live', 'krypton', 'explod', 'get', 'inner', 'space', 'whole', 'inner', 'space', 'notion', 'never', 'explain', 'particularli', 'term', 'peopl', 'get', 'inform', 'outsid', 'world', 'know', 'superman', 'went', 'know', 'took', 'name', 'clark', 'kent', 'work', 'daili', 'planet', 'kind', 'thing', 'frustrat', 'hell', 'argo', 'citi', 'kept', 'run', 'power', 'sourc', 'call', 'omegahedron', 'one', 'day', 'founder', 'argo', 'citi', 'zaltar', 'peter', 'tool', 'give', 'omegahedron', 'superman', 'cousin', 'kara', 'helen', 'slater', 'use', 'inspir', 'art', 'kara', 'use', 'creat', 'bug', 'like', 'thing', 'immedi', 'crash', 'layer', 'whatev', 'protect', 'citi', 'chao', 'omegahedron', 'sail', 'open', 'doom', 'argonian', 'death', 'three', 'day', 'time', 'someth', 'kara', 'climb', 'pod', 'zaltar', 'design', 'interdimension', 'travel', 'chase', 'omegahedron', 'save', 'world', 'power', 'sourc', 'travel', 'directli', 'earth', 'land', 'bowl', 'dip', 'belong', 'minor', 'leagu', 'witch', 'name', 'selena', 'fay', 'dunaway', 'immedi', 'dip', 'ruin', 'announc', 'desir', 'rule', 'world', 'sometim', 'warlock', 'boyfriend', 'nigel', 'peter', 'cook', 'selena', 'somehow', 'know', 'ball', 'sky', 'help', 'evil', 'plan', 'set', 'achiev', 'world', 'domin', 'selena', 'unawar', 'cours', 'kara', 'also', 'arriv', 'earth', 'arriv', 'supergirl', 'complet', 'appropri', 'costum', 'supergirl', 'emerg', 'bottom', 'lake', 'arriv', 'ok', 'huh', 'omegahedron', 'fall', 'sky', 'supergirl', 'shoot', 'bottom', 'lake', 'attent', 'movi', 'explain', 'sorri', 'suspend', 'disbelief', 'much', 'buy', 'nonsens', 'movi', 'want', 'know', 'need', 'secret', 'ident', 'supergirl', 'basic', 'will', 'school', 'girl', 'outfit', 'complet', 'differ', 'hairstyl', 'pick', 'alia', 'linda', 'lee', 'enrol', 'school', 'given', 'dorm', 'assign', 'happen', 'roommat', 'loi', 'lane', 'sister', 'luci', 'ok', 'coincid', 'one', 'thing', 'folk', 'get', 'realli', 'contriv', 'follow', 'gener', 'girl', 'school', 'hijink', 'complet', 'evil', 'bulli', 'girl', 'mean', 'reason', 'cours', 'even', 'shower', 'scene', 'best', 'moment', 'though', 'come', 'supergirl', 'decid', 'put', 'bra', 'school', 'uniform', 'start', 'stuf', 'sock', 'point', 'rapidli', 'wonder', 'bother', 'nonsens', 'credit', 'give', 'film', 'howev', 'one', 'lengthi', 'elabor', 'sequenc', 'featur', 'evil', 'bulldoz', 'destroy', 'small', 'town', 'search', 'prey', 'well', 'done', 'also', 'commend', 'see', 'gender', 'revers', 'film', 'wherea', 'women', 'power', 'men', 'mere', 'object', 'tool', 'hart', 'bochner', 'die', 'hard', 'particularli', 'fun', 'kara', 'love', 'interest', 'lovesick', 'landscap', 'spell', 'cast', 'selena', 'although', 'unfortun', 'part', 'one', 'film', 'ridicul', 'moment', 'rescu', 'tap', 'menac', 'bumper', 'car', 'numer', 'littl', 'thing', 'bother', 'fli', 'effect', 'never', 'convinc', 'moment', 'obviou', 'helen', 'slater', 'hook', 'wire', 'instead', 'look', 'like', 'fli', 'filmmak', 'want', 'grace', 'pose', 'lot', 'ballet', 'type', 'maneuv', 'also', 'first', 'two', 'human', 'meet', 'supergirl', 'arriv', 'earth', 'pair', 'truck', 'driver', 'rapist', 'immedi', 'attack', 'meet', 'wacki', 'demis', 'also', 'rampant', 'product', 'placement', 'film', 'notabl', 'popey', 'tylenol', 'w', 'root', 'beer', 'latter', 'actual', 'put', 'logo', 'shirt', 'one', 'rapist', 'final', 'one', 'word', 'monster', 'fay', 'dunaway', 'peter', 'tool', 'ham', 'unmerci', 'howev', 'tool', 'overact', 'entertain', 'dunaway', 'annoy', 'charact', 'actual', 'say', 'line', 'seiz', 'one', 'point', 'dunaway', 'henchmen', 'play', 'peter', 'cook', 'brenda', 'vaccaro', 'fare', 'much', 'better', 'vaccaro', 'come', 'best', 'equival', 'ned', 'beatti', 'oti', 'charact', 'first', 'two', 'superman', 'movi', 'helen', 'slater', 'effect', 'portray', 'supergirl', 'innoc', 'naiv', 'charact', 'written', 'abl', 'gaze', 'flower', 'bunni', 'right', 'amount', 'genuin', 'wonder', 'make', 'believ', 'believ', 'asid', 'though', 'still', 'annoy', 'despit', 'neg', 'alon', 'supergirl', 'well', 'like', 'well', 'respect', 'movi', 'mani', 'circl', 'leav', 'good', 'peopl', 'anchor', 'bay', 'entertain', 'actual', 'make', 'peopl', 'interest', 'excit', 'releas', 'dvd', 'releas', 'two', 'separ', 'edit', 'film', 'dvd', 'delight', 'mani', 'peopl', 'includ', 'yet', 'still', 'unclear', 'sinc', 'movi', 'aw', 'first', 'wide', 'avail', 'dvd', 'releas', 'featur', 'intern', 'version', 'film', 'run', '10', 'minut', 'longer', 'print', 'ran', 'u', 'theater', 'movi', 'present', 'origin', 'theatric', 'aspect', 'ratio', '2', '35', '1', 'enhanc', '16x9', 'televis', 'remast', 'full', 'thx', 'glori', 'extra', 'disc', 'includ', 'full', 'length', 'audio', 'commentari', 'track', 'director', 'jeannot', 'szwarc', 'special', 'project', 'consult', 'scott', 'michael', 'bosco', 'excel', '50', 'minut', 'documentari', '1984', 'call', 'supergirl', 'make', 'featur', 'dig', 'workout', 'montag', '5', 'trailer', '3', 'tv', 'spot', 'talent', 'bio', 'depth', 'storyboard', 'accompani', 'score', 'movi', 'assort', 'still', 'galleri', 'transfer', 'intern', 'version', 'incred', 'think', 'watch', 'new', 'releas', 'imag', 'sharp', 'color', 'jump', 'screen', 'pictur', 'free', 'scratch', 'artifact', 'special', 'effect', 'sequenc', 'show', 'wear', 'time', 'much', 'done', 'unless', 'obsess', 'like', 'georg', 'luca', 'new', 'thx', 'approv', 'audio', 'loud', 'clear', 'right', 'place', 'howev', 'hear', 'absurd', 'dialogu', 'mediocr', 'jerri', 'goldsmith', 'score', 'complet', 'unrel', 'john', 'william', 'classic', 'superman', 'theme', 'sound', 'better', 'suit', 'cannon', 'film', 'golan', 'globu', 'product', 'commentari', 'track', 'featur', 'director', 'szwarc', 'project', 'consult', 'bosco', 'step', 'right', 'direct', 'evolut', 'audio', 'commentari', 'bosco', 'seem', 'expert', 'thing', 'supergirl', 'szwarc', 'ensur', 'dead', 'air', 'track', 'bosco', 'probe', 'szwarc', 'inform', 'virtual', 'everyon', 'everyth', 'come', 'onscreen', 'learn', 'great', 'deal', 'film', 'result', 'particip', 'although', 'time', 'tell', 'bosco', 'look', 'specif', 'respons', 'szwarc', 'surpris', 'get', 'respons', 'learn', 'fascin', 'tidbit', 'track', 'notabl', 'almost', 'appear', 'film', 'christoph', 'reev', 'great', 'detail', 'given', 'superman', 'propos', 'role', 'origin', 'script', 'reev', 'wise', 'drop', 'stand', 'superman', 'written', 'earli', 'film', 'hear', 'radio', 'broadcast', 'mention', 'billion', 'light', 'year', 'away', 'sort', 'peac', 'mission', 'sake', 'breviti', 'even', 'get', 'one', 'szwarc', 'also', 'reveal', 'consciou', 'decis', 'salkind', 'everyon', 'els', 'agre', 'supergirl', 'need', 'stand', 'replac', 'scienc', 'fiction', 'superman', 'film', 'cutsi', 'whimsic', 'fantasi', 'nonsens', 'bosco', 'also', 'point', 'scene', 'american', 'releas', 'come', 'along', 'claim', 'origin', 'releas', 'would', 'hit', 'box', 'offic', 'sake', 'breviti', 'even', 'get', 'one', 'anchor', 'bay', 'also', 'releas', 'limit', 'edit', '50', '000', 'copi', 'press', 'two', 'disc', 'set', 'disc', 'one', 'contain', 'everyth', 'mention', 'regular', 'releas', 'disc', 'two', 'featur', 'never', 'seen', 'director', 'cut', 'film', 'run', '138', 'minut', 'long', 'addit', 'mainli', 'consist', 'scene', 'extens', 'add', 'much', 'note', 'except', 'moment', 'lengthen', 'peter', 'tool', 'screen', 'time', 'particularli', 'latter', 'appear', 'print', 'rough', 'spot', 'sound', 'mono', 'fan', 'film', 'care', 'one', 'question', 'though', 'director', 'commentari', 'director', 'cut', 'film', 'never', 'seen', 'supergirl', 'sat', 'watch', 'dvd', 'nine', 'releas', 'theater', 'even', 'smart', 'enough', 'inform', 'parent', 'take', 'see', 'pop', 'dvd', 'player', 'sat', 'easi', 'chair', 'idea', 'expect', 'knew', 'film', 'belov', 'never', 'interest', 'will', 'give', 'shot', 'impress', 'amount', 'work', 'put', 'disc', 'disc', 'began', 'spin', 'rather', 'impress', 'open', 'sequenc', 'made', 'sit', 'wonder', 'assumpt', 'time', 'could', 'wrong', 'come', 'find', 'howev', 'chase', 'digit', 'stereo', 'logo', 'slunk', 'back', 'seat', 'nightmar', 'start', 'end', 'film', 'supergirl', 'fli', 'back', 'lake', 'omegahedron', 'fli', 'back', 'argo', 'citi', 'ugh', 'give', 'surpris', 'even', 'made', 'end', 'film', 'twice', 'three', 'time', 'review', 'disc', 'supergirl', 'definit', 'terrif', 'dvd', 'packag', 'lousi', 'movi', 'pg']\n",
            "predicted label: pos, true label: neg\n",
            "\n",
            "\n",
            "tokens: ['susan', 'granger', 'review', 'mulholland', 'drive', 'univers', 'focu', 'whatev', 'david', 'lynch', 'sell', 'buy', 'writer', 'director', 'blue', 'velvet', 'twin', 'peak', 'come', 'anoth', 'dark', 'mysteri', 'thriller', 'open', 'automobil', 'accid', 'mulholland', 'drive', 'serpentin', 'street', 'twist', 'high', 'hollywood', 'hill', 'daze', 'beauti', 'brunett', 'laura', 'elena', 'har', 'emerg', 'stumbl', 'hill', 'slip', 'unobserv', '30', 'style', 'apart', 'tenant', 'leav', 'trip', 'next', 'morn', 'dewi', 'blond', 'naomi', 'watt', 'deep', 'river', 'ontario', 'arriv', 'la', 'dream', 'stardom', 'suitcas', 'aunt', 'own', 'apart', 'two', 'women', 'meet', 'brunett', 'amnesia', 'blond', 'tri', 'help', 'discov', 'ident', 'along', 'latent', 'lesbian', 'lust', 'meanwhil', 'hotshot', 'director', 'justin', 'theroux', 'whose', 'wife', 'bed', 'poolman', 'forc', 'cast', 'certain', 'actress', 'new', 'film', 'assassin', 'mark', 'pellegrino', 'roam', 'citi', 'tortuou', 'path', 'variou', 'charact', 'other', 'name', 'cooki', 'coco', 'cowboy', 'intersect', 'variou', 'point', 'plot', 'remain', 'elus', 'midway', 'stori', 'lynch', 'brunett', 'blond', 'play', 'two', 'differ', 'women', 'altern', 'realiti', 'leav', 'huge', 'wad', 'cash', 'blue', 'metal', 'key', 'paralyz', 'mogul', 'lot', 'question', 'go', 'unansw', 'elena', 'har', 'particularli', 'naomi', 'watt', 'gift', 'actress', 'effect', 'make', 'subtl', 'switch', 'cannot', 'said', 'former', 'g', 'dancer', 'ann', 'miller', 'stiff', 'self', 'consciou', 'speak', 'like', 'lynch', 'player', 'staccato', 'tone', 'granger', 'movi', 'gaug', '1', '10', 'mulholland', 'drive', 'frenzi', 'frustrat', '4', 'origin', 'design', 'episod', 'tv', 'pilot', 'surreal', 'triumph', 'suspens', 'style', 'substanc', 'packag', 'puzzl', 'sever', 'key', 'piec', 'left']\n",
            "predicted label: pos, true label: neg\n",
            "\n",
            "\n",
            "tokens: ['_star', 'wars_', 'came', 'twenti', 'year', 'ago', 'imag', 'travel', 'throughout', 'star', 'becom', 'commonplac', 'imag', 'millenium', 'falcon', 'move', 'throughout', 'star', 'see', 'constel', 'meteor', 'shower', 'cool', 'space', 'ship', 'han', 'solo', 'goe', 'light', 'speed', 'star', 'chang', 'bright', 'line', 'go', 'toward', 'viewer', 'line', 'converg', 'invis', 'point', 'cool', '_octob', 'sky_', 'offer', 'much', 'simpler', 'imag', '--', 'singl', 'white', 'dot', 'travel', 'horizont', 'across', 'night', 'sky', 'realli', 'forti', 'year', 'ago', 'sputnik', 'launch', 'satellit', 'ever', 'exist', 'becom', 'technolog', 'advanc', 'forgotten', 'nearli', 'two', 'gener', 'ago', 'peopl', 'stood', 'outsid', 'breathlessli', 'see', 'technolog', 'achiev', 'even', 'though', 'russian', 'sound', 'hokey', 'scene', 'occur', 'near', 'begin', '_octob', 'sky_', 'found', 'caught', 'enthusiasm', 'homer', 'hickam', 'play', 'jake', 'gyllenha', 'determin', 'make', 'dent', 'space', 'race', 'becom', 'transfix', 'desir', 'make', 'rocket', 'could', 'fli', 'like', 'best', 'film', 'like', 'know', 'end', 'buy', 'ticket', 'love', 'teacher', 'miss', 'riley', 'play', 'laura', 'dern', 'suggest', 'enter', 'find', 'nation', 'scienc', 'fair', 'know', 'exactli', 'go', 'win', 'joy', 'film', 'like', 'littl', 'detail', 'exampl', 'homer', 'clearli', 'strongest', 'student', 'class', 'strength', 'whatsoev', 'undet', 'vision', 'good', 'organ', 'mean', 'may', 'lose', 'reput', 'associ', 'nerdiest', 'kid', 'school', 'know', 'rocket', 'scienc', 'mean', 'father', 'forbid', 'shoot', 'rocket', 'compani', 'properti', 'entir', 'coal', 'mine', 'town', 'must', 'walk', 'eight', 'mile', 'one', 'way', 'set', 'rocket', 'fail', 'fail', 'fail', 'fail', 'homer', 'dad', 'play', 'chri', 'cooper', 'favorit', 'actor', 'mine', 'sinc', '_lone', 'star_', 'play', 'antithesi', 'charact', 'play', '_matewan_', 'instead', 'organ', 'union', 'ralli', 'head', 'coal', 'miner', 'constantli', 'wring', 'hand', 'union', 'one', 'great', 'unnot', 'perform', 'year', 'note', 'multi', 'dimension', 'charact', 'son', 'last', 'nemesi', 'one', 'like', 'interfer', 'son', 'dream', 'truli', 'love', 'son', 'want', 'see', 'succeed', 'way', 'know', 'coal', 'mine', 'charact', 'see', 'far', 'come', 'revolut', 'modern', 'technolog', 'allow', 'us', 'see', 'societi', 'advanc', 'rate', 'far', 'faster', 'gener', 'us', 'understand', 'gener', 'industri', 'revolut', 'knew', 'way', 'rais', 'famili', 'paradigm', 'shift', 'father', 'feet', 'knew', 'life', '_octob', 'sky_', 'far', 'mere', 'feel', 'good', 'inspir', 'film', 'offer', 'much', 'food', 'thought', 'best', 'type', 'inspir', 'film', 'one', 'take', 'someth', 'btw', 'final', 'imag', 'film', 'simpl', 'strong', 'contrast', 'aforement', 'horizont', 'move', 'white', 'dot', 'night', 'sky', 'background', 'anoth', 'commonplac', 'imag', 'sure', 'must', 'breathtak', 'forti', 'year', 'ago']\n",
            "predicted label: neg, true label: pos\n",
            "\n",
            "\n",
            "tokens: ['warn', 'brit', 'love', 'stori', 'of', 'effemin', 'mild', 'manner', 'karl', 'take', 'beatng', 'bunch', 'ruffian', 'school', 'bath', 'shower', 'genit', 'tuck', 'leg', 'manner', 'pose', 'transvestit', 'save', 'prentic', 'appear', 'respect', 'bar', 'protect', 'attitud', 'toward', 'karl', 'averag', 'street', 'wise', 'punk', 'make', 'ensu', 'scene', 'end', 'boy', 'ridicul', 'unfairli', 'expel', 'school', 'see', 'anoth', 'eighteen', 'yea', 'time', 'prentic', 'matur', 'somewhat', 'loveabl', 'brash', 'bull', 'hea', 'goon', 'screw', 'job', 'relationship', 'game', 'cling', 'onto', 'perenni', 'adolesc', 'via', 'lether', 'jacket', 'motorcycl', 'punk', 'rock', 'music', 'karl', 'hand', 'grown', 'becom', 'kim', 'drabbi', 'insecur', 'woman', 'work', 'vers', 'writer', 'greet', 'card', 'compani', 'chanc', 'bring', 'two', 'togeth', 'love', 'inevit', 'blossom', 'quirki', 'british', 'romant', 'comedi', 'skew', 'littl', 'stori', 'larg', 'point', 'think', 'go', 'get', 'cheap', 'laugh', 'gender', 'bend', 'theme', 'see', 'birdcag', 'wong', 'foo', 'priscilla', 'queen', 'desert', 'go', 'home', 'brood', 'maladjust', 'sexual', 'low', 'life', 'deadbeat', 'kim', 'post', 'oper', 'transsexu', 'thank', 'much', 'new', 'born', 'woman', 'purpos', 'confus', 'sensit', 'deserv', 'treatment', 'touch', 'delicaci', 'person', 'give', 'cheap', 'laugh', 'day', 'rather', 'enjoy', 'obviou', 'camp', 'endur', 'shallow', 'explor', 'soft', 'heart', 'transsexu', 'love', 'stori', 'differ', 'girl', 'come', 'promis', 'enough', 'proposit', 'noth', 'surpass', 'mere', 'workmanlik', 'refus', 'make', 'audienc', 'realli', 'uncomfort', 'notion', 'kim', 'prentic', 'relat', 'ship', 'least', 'feel', 'uncomfort', 'prentic', 'exampl', 'seem', 'give', 'hiself', 'much', 'grief', 'becom', 'attract', 'kim', 'despit', 'fight', 'pick', 'beer', 'drink', 'macho', 'kind', 'guy', 'kim', 'insecur', 'fear', 'alway', 'save', 'truli', 'disturb', 'victimis', 'film', 'instead', 'draw', 'basic', 'wimpi', 'love', 'stori', 'film', 'manag', 'lobotomis', 'lightweight', 'comedi', 'doom', 'could', 'still', 'hack', 'requisit', 'endear', 'coupl', 'either', 'sinc', 'even', 'mr', 'doubtfir', 'sexier', 'steven', 'mackintosh', 'kim', 'foyl', 'either', 'sex', 'plainli', 'simper', 'wanker', 'nobodi', 'like', 'prentic', 'though', 'never', 'explain', 'rupert', 'grave', 'give', 'us', 'heroic', 'perform', 'prentic', 'guy', 'never', 'grew', 'bafta', 'prize', 'convinc', 'portray', 'rabid', 'fan', 'buzzcock', 'concert', 'win', 'guess', 'reason', 'explain', 'screw', 'oddli', 'enough', 'grow', 'like', 'show', 'want', 'happi', 'end', 'film', 'fail', 'either', 'snide', 'underl', 'kim', 'offic', 'get', 'come', 'uppanc', 'misogynist', 'polic', 'offic', 'beat', 'prentic', 'get', 'come', 'uppanc', 'coupl', 'prevail', 'disagre', 'societi', 'come', 'togeth', 'kim', 'apart', 'kim', 'sister', 'impot', 'sarg', 'type', 'husband', 'kiss', 'make', 'tiff', 'tv', 'movi', 'sub', 'plot', 'incident', 'juxtaposit', 'best', 'kim', 'shed', 'insecur', 'learn', 'ride', 'motorcycl', 'wear', 'leather', 'make', 'prentic', 'ride', 'pilion', 'awwww', 'terrif', 'closur', 'happi', 'end', 'differnt', 'girl', 'anyway', 'figur', 'one', 'let', 'know', 'fli', 'inkpot', 'rate', 'system', 'wait', 'tv2', 'broadcast', 'littl', 'creaki', 'still', 'better', 'stay', 'home', 'gotcha', 'pretti', 'good', 'bring', 'friend', 'amaz', 'potent', 'stuff', 'perfect', 'see', 'twice']\n",
            "predicted label: pos, true label: neg\n",
            "\n",
            "\n",
            "tokens: ['present', 'day', 'hanoi', 'three', 'sister', 'reflect', 'parent', 'relationship', 'tri', 'defin', 'writer', 'director', 'tran', 'anh', 'hung', 'scent', 'green', 'papaya', 'vertic', 'ray', 'sun', 'youngest', 'sister', 'lien', 'tran', 'nu', 'yen', 'khe', 'greet', 'day', 'hai', 'ngo', 'quanq', 'hai', 'brother', 'room', 'relentlessli', 'flirt', 'morn', 'ritual', 'tai', 'chi', 'lou', 'reed', 'preced', 'cross', 'street', 'eldest', 'sister', 'suong', 'nguyen', 'nhu', 'quynh', 'cafe', 'lien', 'work', 'breakfast', 'three', 'sister', 'lovingli', 'prepar', 'mother', 'memori', 'banquet', 'tradit', 'delicaci', 'prepar', 'much', 'look', 'tast', 'giggl', 'differ', 'sex', 'famili', 'break', 'banquet', 'learn', 'suong', 'young', 'son', 'littl', 'monkey', 'dote', 'upon', 'distanc', 'develop', 'husband', 'quoc', 'chu', 'ngoc', 'hung', 'leav', 'one', 'habitu', 'botan', 'photographi', 'trip', 'middl', 'sister', 'khanh', 'tran', 'manh', 'cuong', 'tell', 'writer', 'husband', 'pregnant', 'leav', 'trip', 'saigon', 'research', 'tuan', 'mysteri', 'man', 'mother', 'law', 'past', 'lien', 'keep', 'crawl', 'hai', 'bed', 'night', 'caus', 'fall', 'initi', 'seren', 'wit', 'start', 'turn', 'soap', 'opera', 'come', 'full', 'circl', 'one', 'month', 'later', 'three', 'women', 'prepar', 'father', 'memori', 'banquet', 'vertic', 'ray', 'sun', 'contempl', 'piec', 'strong', 'passion', 'exist', 'underneath', 'calm', 'exterior', 'sli', 'humor', 'self', 'deceit', 'harsh', 'truth', 'exist', 'cyclic', 'screenplay', 'tran', 'anh', 'hung', 'cinematograph', 'mark', 'lee', 'ping', 'bin', 'flower', 'shanghai', 'linger', 'repeat', 'small', 'gestur', 'everyday', 'life', 'wife', 'wash', 'husband', 'hand', 'woman', 'make', 'water', 'danc', 'bowl', 'man', 'pull', 'lover', 'face', 'part', 'small', 'space', 'room', 'alley', 'courtyard', 'creat', 'intimaci', 'color', 'green', 'symbol', 'life', 'tranquil', 'use', 'heavili', 'film', 'visual', 'style', 'laps', 'yellowish', 'hue', 'blue', 'writer', 'director', 'tran', 'anh', 'hung', 'film', 'recal', 'ang', 'lee', 'eat', 'drink', 'man', 'woman', 'anoth', 'tale', 'three', 'sister', 'vari', 'modern', 'come', 'togeth', 'tradit', 'parent', 'lee', 'tradit', 'stori', 'driven', 'film', 'result', 'radic', 'chang', 'concern', 'vertic', 'ray', 'sun', 'charact', 'strive', 'peac', 'harmoni', 'watch', 'like', 'trail', 'fingertip', 'stream', 'cool', 'spring', 'day']\n",
            "predicted label: neg, true label: pos\n",
            "\n",
            "\n",
            "tokens: ['first', 'admit', 'expect', 'much', 'emperor', 'new', 'groov', 'celin', 'dion', 'esqu', 'song', 'danc', 'number', 'cuddli', 'charact', 'becom', 'cross', 'promot', 'devic', 'mcdonald', 'happi', 'meal', 'hollywood', 'star', 'tri', 'gain', 'credibl', 'ad', 'voic', 'job', 'resum', 'dead', 'wrong', 'thing', 'emperor', 'new', 'groov', 'funni', 'damn', 'funni', 'cute', 'charact', 'film', 'danger', 'one', 'script', 'fast', 'furiou', 'sing', 'danc', 'anim', 'clean', 'would', 'give', 'walt', 'warm', 'feel', 'humor', 'dri', 'aim', 'cynic', 'good', 'us', 'stori', 'ridicul', 'weird', 'come', 'strang', 'acid', 'trip', 'involv', 'talk', 'cross', 'dress', 'llama', 'schizophrenia', 'plot', 'tri', 'follow', 'along', 'set', 'mythic', 'kingdom', 'film', 'follow', 'adventur', 'kuzco', 'david', 'spade', 'arrog', 'egocentr', 'emperor', 'kuzco', 'fire', 'power', 'hungri', 'advisor', 'yzma', 'eartha', 'kitt', 'assist', 'kronk', 'seinfeld', 'patrick', 'warburton', 'chang', 'llama', 'kuzco', 'get', 'strand', 'jungl', 'must', 'reli', 'pacha', 'john', 'goodman', 'llama', 'herder', 'whose', 'home', 'replac', 'kuzco', 'water', 'world', 'theme', 'park', 'save', 'fun', 'start', 'next', 'sixti', 'minut', 'audienc', 'treat', 'someth', 'unheard', 'disney', 'film', 'day', 'imagin', 'yzma', 'voic', 'talent', 'singer', 'eartha', 'kitt', 'look', 'like', 'cross', 'norma', 'desmond', 'joan', 'crawford', 'realli', 'old', 'vega', 'showgirl', 'dialogu', 'oblivi', 'assist', 'kronk', 'quick', 'sharp', 'feel', 'like', 'episod', 'seinfeld', 'would', 'make', 'sens', 'david', 'spade', 'john', 'goodman', 'charact', 'interact', 'make', 'feel', 'like', 'watch', 'old', 'hope', 'crosbi', 'film', 'spade', 'beyond', 'dri', 'humor', 'shoot', 'belt', 'sever', 'occas', 'laugh', 'loud', 'sever', 'time', 'needless', 'say', 'extrem', 'rare', 'thing', 'disney', 'screen', 'day', 'best', 'part', 'emperor', 'new', 'groov', 'audaci', 'film', 'sheer', 'weird', 'factor', 'high', 'cross', 'dress', 'schizophrenia', 'dualiti', 'soul', 'peopl', 'turn', 'everi', 'anim', 'noah', 'boat', 'llama', 'cpr', 'homoerot', 'tendenc', 'emperor', 'theme', 'song', 'singer', 'michael', 'jackson', 'danc', 'move', 'guy', 'commun', 'squirrel', 'odditi', 'make', 'film', 'true', 'orgin', 'simpli', 'put', 'emperor', 'new', 'groov', 'one', 'best', 'children', 'film', 'holiday', 'season', 'kid', 'enjoy', 'adventur', 'parent', 'enjoy', 'stori', 'dialogu', 'tough', 'thing', 'pull', 'singl', 'packag', 'emperor', 'manag']\n",
            "predicted label: neg, true label: pos\n",
            "\n",
            "\n",
            "tokens: ['shakespear', 'love', 'quit', 'possibl', 'enjoy', 'period', 'piec', 'ever', 'made', 'silver', 'screen', 'humor', 'romant', 'uniqu', 'blend', 'success', 'entertain', 'audienc', 'nearli', '2', 'half', 'hour', 'occupi', 'howev', 'say', 'good', 'film', 'qualiti', 'product', 'anyth', 'sort', 'shakespear', 'love', 'incred', 'cheap', 'illus', 'truli', 'pan', 'littl', 'qualiti', 'origin', 'work', 'finest', 'sign', 'may', 'plot', 'look', 'back', 'seem', 'littl', 'thin', 'predict', 'plot', 'carri', 'portray', 'peopl', 'rever', 'histori', 'book', 'philip', 'henslow', 'geoffrey', 'rush', 'own', '1', '2', 'theatr', 'london', 'peak', 'royal', 'theatr', 'era', 'queen', 'elizabeth', 'recent', 'dame', 'judi', 'dench', 'appropri', 'enough', 'queen', 'elizabeth', 'ii', 'much', 'fan', 'howev', 'directli', 'quot', 'film', 'cash', 'flow', 'problem', 'long', 'set', 'event', 'becom', 'appar', 'entir', 'life', 'depend', 'next', 'show', 'well', 'enough', 'pay', 'debt', 'mr', 'henslow', 'employ', 'young', 'playwright', 'william', 'shakespear', 'joseph', 'fienn', 'pen', 'comed', 'product', 'howev', 'young', 'writer', 'sever', 'case', 'writer', 'block', 'blame', 'fact', 'love', 'life', 'struggl', 'well', 'titl', 'mind', 'romeo', 'ethel', 'pirat', 'daughter', 'even', 'joke', 'lose', 'steam', 'seem', 'put', 'word', 'paper', 'hollywood', 'could', 'long', 'set', 'twist', 'event', 'meet', 'viola', 'de', 'lessep', 'gwyneth', 'paltrow', 'fall', 'madli', 'love', 'thu', 'cure', 'writer', 'block', 'mani', 'littl', 'issu', 'mr', 'henslow', 'encount', 'pan', 'much', 'ado', 'noth', 'first', 'realiz', 'reach', 'watch', 'film', 'one', 'messag', 'given', 'show', 'alway', 'credit', 'author', 'iron', 'truer', 'great', 'scene', 'sweep', 'audienc', 'away', 'scene', 'fit', 'plot', 'rather', 'recit', 'shakespearean', 'line', 'actor', 'play', 'actor', 'one', 'breathtak', 'moment', 'film', 'involv', 'charact', 'shakespear', 'queen', 'elizabeth', 'even', 'theatr', 'owner', 'rather', '2', 'young', 'children', 'name', 'romeo', 'juliet', 'chose', 'end', 'live', 'name', 'love', 'offend', 'fact', 'marc', 'norman', 'tom', 'stoppard', 'credit', 'write', 'product', 'name', 'william', 'shakespear', 'seen', 'beyond', 'charact', 'name', 'credit', 'act', 'entertain', 'yet', 'poor', 'film', 'often', 'thin', 'point', 'would', 'surviv', 'even', 'queen', 'elizabeth', 'theatr', 'joseph', 'fienn', 'may', 'worst', 'fall', 'though', 'tragic', 'unbeliev', 'comic', 'bad', 'gwyneth', 'paltrow', 'littl', 'satisfactori', 'lead', 'posit', 'well', 'howev', 'support', 'cast', 'almost', 'save', 'day', 'geoffrey', 'rush', 'noth', 'short', 'incred', 'judi', 'dench', 'breathtak', 'seem', 'shown', 'proven', 'actor', 'could', 'surviv', 'film', 'weak', 'link', 'also', 'find', 'good', 'perform', 'ben', 'affleck', 'first', 'real', 'role', 'sinc', 'good', 'hunt', 'armageddon', 'qualifi', 'real', 'act', 'rupert', 'everett', 'cute', 'small', 'part', 'well', 'even', 'could', 'save', 'sad', 'excus', 'film', 'remain', 'plagu', 'poor', 'perform', 'said', 'done', 'shakespear', 'love', 'worth', 'trip', 'want', 'entertain', 'howev', 'film', 'kindli', 'point', 'entertain', 'may', 'fun', 'necessarili', 'qualiti', 'certainli', 'qualiti', 'perhap', 'may', 'best', 'compar', 'john', 'grisham', 'novel', 'dear', 'friend', 'mine', 'often', 'compar', 'thing', 'work', 'simpli', 'put', 'far', 'fetch', 'poorli', 'craft', 'entertain']\n",
            "predicted label: pos, true label: neg\n",
            "\n",
            "\n",
            "tokens: ['well', 'check', 'score', 'card', 'done', 'holiday', 'weekend', 'read', 'good', 'idea', '0', 'dumb', 'idea', '1', 'know', 'think', 'decid', 'watch', 'movi', 'defens', 'say', 'someon', 'els', 'urg', 'see', 'film', 'talk', 'night', 'roxburi', 'one', 'offer', 'base', 'saturday', 'night', 'live', 'skit', 'wayn', 'garth', 'two', 'rather', 'meet', 'doug', 'steve', 'butabi', 'actor', 'name', 'worth', 'mention', 'two', 'etern', 'partyer', 'whose', 'greatest', 'ambit', 'life', 'seem', 'find', 'way', 'get', 'hottest', 'night', 'club', 'citi', 'roxburi', 'drive', 'dad', 'bmw', 'don', 'metal', 'disco', 'suit', 'right', 'miami', 'vice', 'tri', 'bribe', 'bouncer', 'get', 'club', 'met', 'friend', 'washington', 'friend', 'roosevelt', 'confid', 'say', 'pull', 'spare', 'pocket', 'chang', 'second', 'greatest', 'ambit', 'seem', 'ooz', 'mani', 'silli', 'pick', 'line', 'humanli', 'possibl', 'order', 'start', 'convers', 'girl', 'let', 'see', 'label', 'thought', 'made', 'heaven', 'say', 'one', 'brother', 'like', 'loser', 'fail', 'either', 'much', 'film', 'howev', 'fate', 'would', 'accident', 'meet', 'ever', 'happen', 'richard', 'grieco', 'give', 'import', 'ticket', 'get', 'sad', 'live', 'take', 'whole', 'new', 'direct', 'make', 'import', 'contact', 'club', 'owner', 'believ', 'two', 'brother', 'uncanni', 'insight', 'club', 'scene', 'mistaken', 'rich', 'swinger', 'two', 'voluptu', 'young', 'women', 'newfound', 'popular', 'impress', 'father', 'plan', 'unfortun', 'thing', 'film', 'one', 'joke', 'movi', 'brother', 'joke', 'actual', '10', 'mintu', 'worth', 'toler', 'stuff', 'ala', 'long', 'tv', 'way', 'short', 'featur', 'length', 'film', 'thu', 'enough', 'materi', 'sustain', '83', 'minut', 'movi', 'plot', 'found', 'everyth', 'two', 'seem', 'culmin', 'opportun', 'execut', 'trademark', 'move', 'snap', 'head', 'unison', 'funki', 'beat', 'haddaway', 'europop', 'song', 'love', 'amaz', 'none', 'two', 'suffer', 'whiplash', 'take', 'film', 'time', 'subplot', 'involv', 'daughter', 'businessman', 'next', 'door', 'want', 'marri', 'doug', 'creat', 'friction', 'brother', 'unlik', 'pair', 'also', 'agit', 'audienc', 'member', 'well', 'educ', 'forebear', 'witch', 'meanwhil', 'doug', 'complet', 'loser', 'two', 'ever', 'get', 'togeth', 'suppos', 'ever', 'figur', 'answer', 'question', 'figur', 'decid', 'go', 'see', 'movi', 'look', 'entertain', 'find', 'roxburi']\n",
            "predicted label: pos, true label: neg\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# These are some problems may cause error in this classification task:\n",
        "**Lack of Context:** Naive Bayes treats each feature (word) independently given the class. It does not consider the order of words, so it might miss the context that can change the sentiment of a phrase. For example, \"not bad\" might be classified incorrectly if \"not\" and \"bad\" are usually associated with negative sentiments separately.\n",
        "<br>\n",
        "**Uncommon Words or Phrases:** If certain words or phrases that significantly alter sentiment are rare in the training set, the classifier might not have a good estimation of their effects.\n",
        "<br>\n",
        "**Sarcasm or Irony:** These are particularly difficult for algorithms to detect because they often involve stating something that is the opposite of what is meant, which requires understanding context, culture, and often the tone in which something was said.\n"
      ],
      "metadata": {
        "id": "BBg5qWmg4p28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Submission Instructions:\n"
      ],
      "metadata": {
        "id": "Nfl8UA42Gqjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "3.Clearly present the results of your parameter tuning in the notebook.\n",
        "\n",
        "4.Provide a brief summary of your findings and insights in the conclusion section.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Experiment with various seed texts to showcase the diversity of generated text.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ],
      "metadata": {
        "id": "75kVTQX6GsCn"
      }
    }
  ]
}