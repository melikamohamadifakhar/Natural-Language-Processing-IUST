{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Q1***"
      ],
      "metadata": {
        "id": "eUBz8VDL0Qma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "key steps involved in starting and developing an ASR product:\n",
        "\n",
        "### 1. Conceptualization and Research\n",
        "- **Identify Use Case:** Define target audience and specific problems the ASR product will solve.\n",
        "- **Market Research:** Analyze competitors and market needs.\n",
        "- **Feasibility Study:** Assess technical feasibility and potential challenges.\n",
        "\n",
        "### 2. Data Collection and Preparation\n",
        "- **Data Collection:** Gather diverse audio recordings.\n",
        "- **Data Annotation:** Transcribe audio data for training.\n",
        "- **Data Preprocessing:** Clean and normalize the data.\n",
        "\n",
        "### 3. Model Development\n",
        "- **Choose/Develop ASR Framework:** Decide on using existing frameworks or building a custom solution.\n",
        "- **Model Training:** Train the ASR model with collected data.\n",
        "- **Evaluation and Testing:** Evaluate performance using metrics like Word Error Rate (WER).\n",
        "\n",
        "### 4. Development of Supporting Components\n",
        "- **Front-End Development:** Design user-friendly interface.\n",
        "- **Back-End Development:** Build server-side infrastructure.\n",
        "- **Integration:** Integrate ASR model with system components.\n",
        "\n",
        "### 5. Deployment\n",
        "- **Scalability and Optimization:** Ensure system can handle varying loads.\n",
        "- **Cloud/On-Premise Deployment:** Choose deployment strategy.\n",
        "- **Continuous Monitoring:** Track performance and usage.\n",
        "\n",
        "### 6. Continuous Improvement\n",
        "- **User Feedback:** Collect and act on feedback.\n",
        "- **Model Updates:** Regularly update the model with new data.\n",
        "- **Feature Enhancements:** Develop new features based on user needs.\n",
        "\n",
        "### 7. Compliance and Security\n",
        "- **Data Privacy:** Ensure compliance with regulations.\n",
        "- **Security Measures:** Protect user data and prevent unauthorized access.\n",
        "\n",
        "### Tools and Technologies\n",
        "- **ASR Frameworks:** Kaldi, DeepSpeech, Wav2Vec\n",
        "- **Programming Languages:** Python, C++\n",
        "- **Deep Learning Frameworks:** TensorFlow, PyTorch\n",
        "- **Cloud Platforms:** AWS, Google Cloud, Azure\n",
        "- **Data Annotation Tools:** Labelbox, Doccano\n",
        "\n"
      ],
      "metadata": {
        "id": "9BSZHPCa0BjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Q2***"
      ],
      "metadata": {
        "id": "mScglnIv00vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "two established methods and one novel approach:\n",
        "\n",
        "### Established Methods\n",
        "\n",
        "1. **Majority Voting (Ensemble Method)**\n",
        "   - **Steps:**\n",
        "     1. Collect outputs from all models.\n",
        "     2. Rank outputs by frequency.\n",
        "     3. Select the top 16 most frequent outputs.\n",
        "\n",
        "2. **Weighted Averaging (Weighted Ensemble)**\n",
        "   - **Steps:**\n",
        "     1. Assign weights to each model based on their performance.\n",
        "     2. Score outputs by multiplying them by their model weights.\n",
        "     3. Aggregate scores for each output.\n",
        "     4. Select the top 16 outputs with the highest scores.\n",
        "\n",
        "### Novel Method\n",
        "\n",
        "3. **Hybrid Consensus Method**\n",
        "   - **Steps:**\n",
        "     1. Perform initial majority voting to find the top 20 outputs.\n",
        "     2. Apply weighted averaging to refine the top 20 outputs.\n",
        "     3. Select the top 16 outputs based on the refined weighted scores."
      ],
      "metadata": {
        "id": "C126iVoT02ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Q3***"
      ],
      "metadata": {
        "id": "QYczEeHl2b-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Normalizer:** Converts text to a standard format (e.g., lowercasing, removing punctuation).\n",
        "2. **Formalizer:** Converts informal or slang text to formal language.\n",
        "3. **Lemmatizer:** Reduces words to their base or dictionary form (considering context).\n",
        "4. **Stemmer:** Reduces words to their root form by removing affixes (without context).\n",
        "5. **Chunker:** Groups words into phrases (e.g., noun phrases, verb phrases).\n",
        "6. **Tagger:** Assigns tags to words (e.g., grammatical tags).\n",
        "7. **POS Tagger:** Specifically assigns part-of-speech tags to words.\n",
        "8. **Parser:** Analyzes the grammatical structure of sentences, creating parse trees.\n",
        "9. **Word Embedder:** Converts words into numerical vectors capturing semantic meanings.\n",
        "10. **Embedder:** Converts larger text units (e.g., sentences, documents) into numerical vectors.\n"
      ],
      "metadata": {
        "id": "l3nroa2H2PQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Q4***"
      ],
      "metadata": {
        "id": "pN8SqZe-5Z9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am_zNfoEzPZM",
        "outputId": "2f676dd6-25cc-4dfa-ef90-eff50c20a1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/roshan-research/hazm.git\n",
            "  Cloning https://github.com/roshan-research/hazm.git to /tmp/pip-req-build-3c5c31l2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/roshan-research/hazm.git /tmp/pip-req-build-3c5c31l2\n",
            "  Resolved https://github.com/roshan-research/hazm.git to commit e6705fe99d6d9248d941c1c1693d2ea46bb29cc5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm==0.10.0) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm==0.10.0) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm==0.10.0) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm==0.10.0) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm==0.10.0) (0.9.10)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm==0.10.0) (1.2.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm==0.10.0) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm==0.10.0) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (4.66.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm==0.10.0) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/roshan-research/hazm.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Initialize Hazm normalizer\n",
        "normalizer = hazm.Normalizer()\n",
        "\n",
        "# Function to normalize text using Hazm\n",
        "def hazm_normalize(text):\n",
        "    return normalizer.normalize(text)\n",
        "\n",
        "# Function to calculate Character Error Rate (CER)\n",
        "def cer(prediction, reference):\n",
        "    s = SequenceMatcher(None, prediction, reference)\n",
        "    return 1 - s.ratio()\n",
        "\n",
        "# Function to calculate Word Error Rate (WER)\n",
        "def wer(prediction, reference):\n",
        "    prediction_words = prediction.split()\n",
        "    reference_words = reference.split()\n",
        "    sm = SequenceMatcher(None, prediction_words, reference_words)\n",
        "    return 1 - sm.ratio()\n",
        "\n",
        "# Read HARF output and true transcription from files\n",
        "with open(\"/content/HarfOutp.txt\", 'r', encoding='utf-8') as file:\n",
        "    harf_output = file.read()\n",
        "\n",
        "with open(\"/content/true_output.txt\", 'r', encoding='utf-8') as file:\n",
        "    true_transcription = file.read()\n",
        "\n",
        "# Normalize the transcriptions using Hazm\n",
        "normalized_harf_output = hazm_normalize(harf_output)\n",
        "normalized_true_transcription = hazm_normalize(true_transcription)\n",
        "\n",
        "# Calculate CER and WER\n",
        "cer_value = cer(normalized_harf_output, normalized_true_transcription)\n",
        "wer_value = wer(normalized_harf_output, normalized_true_transcription)\n",
        "\n",
        "cer_value, wer_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLm6cvi05_BR",
        "outputId": "b48d8f8e-d5bc-4ff8-bf95-5e3392f62ef3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.44252688748347446, 0.27552552552552556)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Q5***"
      ],
      "metadata": {
        "id": "UIpj3Js1AGQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "# Initialize Hazm tools\n",
        "normalizer = hazm.Normalizer()\n",
        "tokenizer = hazm.word_tokenize\n",
        "tagger = hazm.POSTagger(model=\"/content/pos_tagger.model\")"
      ],
      "metadata": {
        "id": "MsvtG_8_6tGx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and POS tag the transcriptions\n",
        "harf_tokens = tokenizer(normalized_harf_output)\n",
        "true_tokens = tokenizer(normalized_true_transcription)\n",
        "\n",
        "harf_tags = tagger.tag(harf_tokens)\n",
        "true_tags = tagger.tag(true_tokens)\n",
        "\n",
        "# Count the number of verbs and adverbs in each transcription\n",
        "def count_verbs_adverbs(tags):\n",
        "    verb_count = 0\n",
        "    adverb_count = 0\n",
        "    for word, tag in tags:\n",
        "        if tag.startswith('V'):  # Verb tags usually start with 'V'\n",
        "            verb_count += 1\n",
        "        elif tag == 'ADV':  # Adverb tag\n",
        "            adverb_count += 1\n",
        "    return verb_count, adverb_count\n",
        "\n",
        "harf_verb_count, harf_adverb_count = count_verbs_adverbs(harf_tags)\n",
        "true_verb_count, true_adverb_count = count_verbs_adverbs(true_tags)\n",
        "\n",
        "print(f\"HARF Output - Verbs: {harf_verb_count}, Adverbs: {harf_adverb_count}\")\n",
        "print(f\"True Transcription - Verbs: {true_verb_count}, Adverbs: {true_adverb_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRu6QnKR7y_w",
        "outputId": "0ca407aa-ec17-4742-c842-117c486ab1e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HARF Output - Verbs: 294, Adverbs: 78\n",
            "True Transcription - Verbs: 290, Adverbs: 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Q6***"
      ],
      "metadata": {
        "id": "McnMyUjP_wjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "# Function to find the most frequent verb stem\n",
        "def most_frequent_verb_stem(text):\n",
        "    # Normalize the text\n",
        "    normalized_text = normalizer.normalize(text)\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(normalized_text)\n",
        "    # POS tag the tokens\n",
        "    tags = tagger.tag(tokens)\n",
        "\n",
        "    # Dictionary to count verb stems\n",
        "    verb_stem_count = {}\n",
        "\n",
        "    # Lemmatize and count verb stems\n",
        "    for word, tag in tags:\n",
        "        if tag.startswith('V'):  # Verb tags usually start with 'V'\n",
        "            lemma = lemmatizer.lemmatize(word)\n",
        "            # Extract the stem (part after '#')\n",
        "            stem = lemma.split('#')[-1]\n",
        "            if stem in verb_stem_count:\n",
        "                verb_stem_count[stem] += 1\n",
        "            else:\n",
        "                verb_stem_count[stem] = 1\n",
        "\n",
        "    # Find the most frequent verb stem\n",
        "    most_frequent_stem = max(verb_stem_count, key=verb_stem_count.get)\n",
        "    return most_frequent_stem, verb_stem_count[most_frequent_stem]\n",
        "\n",
        "# Find the most frequent verb stem in each file\n",
        "harf_most_frequent_stem, harf_count = most_frequent_verb_stem(harf_output)\n",
        "true_most_frequent_stem, true_count = most_frequent_verb_stem(true_transcription)\n",
        "\n",
        "print(f\"HARF Output - Most Frequent Verb Stem: '{harf_most_frequent_stem}', Count: {harf_count}\")\n",
        "print(f\"True Transcription - Most Frequent Verb Stem: '{true_most_frequent_stem}', Count: {true_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awRUCvIT-mzD",
        "outputId": "d6cf3b5a-3736-442e-9099-8992fbacfeb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HARF Output - Most Frequent Verb Stem: 'است', Count: 46\n",
            "True Transcription - Most Frequent Verb Stem: 'کن', Count: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Q7***"
      ],
      "metadata": {
        "id": "uBHlTIMAA4Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mega.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biLsBRqNHAYw",
        "outputId": "2c523066-ee22-4cdb-ab8e-02c999676795"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mega.py in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: requests>=0.10 in /usr/local/lib/python3.10/dist-packages (from mega.py) (2.31.0)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.9.6 in /usr/local/lib/python3.10/dist-packages (from mega.py) (3.20.0)\n",
            "Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.10/dist-packages (from mega.py) (1.0.1)\n",
            "Requirement already satisfied: tenacity<6.0.0,>=5.1.5 in /usr/local/lib/python3.10/dist-packages (from mega.py) (5.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=0.10->mega.py) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from tenacity<6.0.0,>=5.1.5->mega.py) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mega import Mega\n",
        "mega = Mega()\n",
        "m = mega.login()"
      ],
      "metadata": {
        "id": "Y6k5YoZJHHbX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.download_url('https://mega.nz/file/GqZUlbpS#XRYP5FHbPK2LnLZ8IExrhrw3ZQ-jclNSVCz59uEhrxY')"
      ],
      "metadata": {
        "id": "TuxVcAUsO2Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip fasttext_model.zip"
      ],
      "metadata": {
        "id": "OjALsuB1HeEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the word embedding model (assuming the model file 'word2vec.bin' is correctly placed)\n",
        "word_embedding = WordEmbedding(model_type='fasttext', model_path=\"/content/fasttext_skipgram_300.bin\")\n",
        "\n",
        "# Function to find the unrelated word in each group of words\n",
        "def find_unrelated_words(groups):\n",
        "    unrelated_words = []\n",
        "    for group in groups:\n",
        "        unrelated_word = word_embedding.doesnt_match(group)\n",
        "        unrelated_words.append(unrelated_word)\n",
        "    return unrelated_words\n",
        "\n",
        "# Example list of lists containing 5 Farsi words each\n",
        "word_groups = [\n",
        "    ['سلام', 'درود', 'خداحافظ', 'بدرود', 'درخت'],\n",
        "    ['ساعت', 'پلنگ', 'شیر', 'ببر', 'گربه'],\n",
        "    ['کتاب', 'دفتر', 'خودکار', 'مداد', 'سیب'],\n",
        "    ['آسمان', 'دختر', 'خورشید', 'ماه', 'ستاره'],\n",
        "    ['دوچرخه', 'ماشین', 'موتور', 'قطار', 'درخت']\n",
        "]\n",
        "\n",
        "# Find and print the unrelated word in each group\n",
        "unrelated_words = find_unrelated_words(word_groups)\n",
        "for group, unrelated_word in zip(word_groups, unrelated_words):\n",
        "    print(f\"In the group {group}, the unrelated word is: '{unrelated_word}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU37aWWP_7Uo",
        "outputId": "94c875f2-8708-47d6-e886-ac818a355818"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the group ['سلام', 'درود', 'خداحافظ', 'بدرود', 'درخت'], the unrelated word is: 'درخت'\n",
            "In the group ['ساعت', 'پلنگ', 'شیر', 'ببر', 'گربه'], the unrelated word is: 'ساعت'\n",
            "In the group ['کتاب', 'دفتر', 'خودکار', 'مداد', 'سیب'], the unrelated word is: 'سیب'\n",
            "In the group ['آسمان', 'دختر', 'خورشید', 'ماه', 'ستاره'], the unrelated word is: 'دختر'\n",
            "In the group ['دوچرخه', 'ماشین', 'موتور', 'قطار', 'درخت'], the unrelated word is: 'درخت'\n"
          ]
        }
      ]
    }
  ]
}