{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALL DADMATOOLS"
      ],
      "metadata": {
        "id": "mJ42Jm6TmDWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExfNZa5Sj9Zr",
        "outputId": "77b14203-256c-4527-8e43-e16433cce633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dadmatools\n",
            "  Downloading dadmatools-2.0.4-py3-none-any.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.0/883.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.3 (from dadmatools)\n",
            "  Downloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.8.1)\n",
            "Requirement already satisfied: folium>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.14.0)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.7.4)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (4.41.1)\n",
            "Requirement already satisfied: h5py>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.9.0)\n",
            "Collecting Deprecated==1.2.6 (from dadmatools)\n",
            "  Downloading Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: hyperopt>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.2.7)\n",
            "Collecting pyconll>=3.1.0 (from dadmatools)\n",
            "  Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
            "Collecting pytorch-transformers>=1.1.0 (from dadmatools)\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting segtok>=1.5.7 (from dadmatools)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.9.0)\n",
            "Collecting supar==1.1.2 (from dadmatools)\n",
            "  Downloading supar-1.1.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (4.3.2)\n",
            "Collecting conllu (from dadmatools)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gdown>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (5.1.0)\n",
            "Collecting py7zr>=0.17.2 (from dadmatools)\n",
            "  Downloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2text (from dadmatools)\n",
            "  Downloading html2text-2024.2.26.tar.gz (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from dadmatools)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dadmatools) (1.25.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dadmatools) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (4.66.4)\n",
            "Collecting langid==1.1.6 (from dadmatools)\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from dadmatools) (3.14.0)\n",
            "Requirement already satisfied: tokenizers>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from dadmatools) (2024.5.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dadmatools) (24.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from dadmatools) (0.1.99)\n",
            "Collecting sacremoses (from dadmatools)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext (from dadmatools)\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kenlm (from dadmatools)\n",
            "  Downloading kenlm-0.2.0.tar.gz (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.4/427.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==2.10.0 (from dadmatools)\n",
            "  Downloading emoji-2.10.0-py2.py3-none-any.whl (457 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.9/457.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated==1.2.6->dadmatools) (1.14.1)\n",
            "Collecting stanza (from supar==1.1.2->dadmatools)\n",
            "  Downloading stanza-1.8.2-py3-none-any.whl (990 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from supar==1.1.2->dadmatools)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium>=0.2.1->dadmatools) (0.7.2)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium>=0.2.1->dadmatools) (3.1.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.3.1->dadmatools) (4.12.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.6.0->dadmatools) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.6.0->dadmatools) (6.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.10.9.7)\n",
            "Collecting texttable (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.15.9 (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading pyzstd-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.8/413.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj<1.1.0,>=1.0.0 (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.1.0 (from py7zr>=0.17.2->dadmatools)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr>=0.17.2->dadmatools) (5.9.5)\n",
            "Collecting boto3 (from pytorch-transformers>=1.1.0->dadmatools)\n",
            "  Downloading boto3-1.34.118-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->dadmatools) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->dadmatools) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (0.9.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (2.7.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0.0->dadmatools) (3.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dadmatools) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dadmatools) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dadmatools) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dadmatools) (2024.2.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.7.0->dadmatools) (0.23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->dadmatools) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->dadmatools) (1.12.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->dadmatools) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.1->dadmatools)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->dadmatools) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->dadmatools)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.1->dadmatools) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.9.1->dadmatools) (0.4.3)\n",
            "Collecting pybind11>=2.2 (from fasttext->dadmatools)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->dadmatools) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->dadmatools) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->dadmatools) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->dadmatools) (2.18.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0.0->dadmatools) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0.0->dadmatools) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.0.0->dadmatools) (0.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.3.1->dadmatools) (2.5)\n",
            "Collecting botocore<1.35.0,>=1.34.118 (from boto3->pytorch-transformers>=1.1.0->dadmatools)\n",
            "  Downloading botocore-1.34.118-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers>=1.1.0->dadmatools)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers>=1.1.0->dadmatools)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->dadmatools) (1.7.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza->supar==1.1.2->dadmatools) (0.10.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->dadmatools) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.118->boto3->pytorch-transformers>=1.1.0->dadmatools) (2.8.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->dadmatools) (1.1.1)\n",
            "Building wheels for collected packages: langid, fasttext, html2text, kenlm\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=f6199e8a77184fceeee9959a7a0ff2ac434486d7dd829dfecc352712dc12df65\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227137 sha256=f2226674ab0b3bab566b480cd395f628f76115a03b7db585f5d76fe8626b5737\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html2text: filename=html2text-2024.2.26-py3-none-any.whl size=33111 sha256=a388c36913d236c7bba28c1d963d48f3fa1e3c765688ae5e7018d3a17c8dfa4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/96/6d/a7eba8f80d31cbd188a2787b81514d82fc5ae6943c44777659\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184430 sha256=99dfcac623bc70447d8a58751a028e643f63b144d373b1af4f1c08c84a8ad577\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/80/e0/18f4148e863fb137bd87e21ee2bf423b81b3ed6989dab95135\n",
            "Successfully built langid fasttext html2text kenlm\n",
            "Installing collected packages: tf-estimator-nightly, texttable, kenlm, brotli, segtok, sacremoses, pyzstd, pyppmd, pycryptodomex, pyconll, pybind11, pybcj, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multivolumefile, langid, jmespath, inflate64, html2text, emoji, dill, Deprecated, conllu, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext, botocore, s3transfer, nvidia-cusolver-cu12, bpemb, boto3, stanza, pytorch-transformers, supar, dadmatools\n",
            "Successfully installed Deprecated-1.2.6 boto3-1.34.118 botocore-1.34.118 bpemb-0.3.5 brotli-1.1.0 conllu-4.5.3 dadmatools-2.0.4 dill-0.3.8 emoji-2.10.0 fasttext-0.9.2 html2text-2024.2.26 inflate64-1.0.0 jmespath-1.0.1 kenlm-0.2.0 langid-1.1.6 multivolumefile-0.2.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 py7zr-0.21.0 pybcj-1.0.2 pybind11-2.12.0 pyconll-3.2.0 pycryptodomex-3.20.0 pyppmd-1.1.0 pytorch-transformers-1.2.0 pyzstd-0.16.0 s3transfer-0.10.1 sacremoses-0.1.1 segtok-1.5.11 stanza-1.8.2 supar-1.1.2 texttable-1.7.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "! pip install dadmatools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "BKQnWj5AmJwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dadmatools.normalizer import Normalizer\n",
        "from dadmatools.pipeline.informal2formal.main import Informal2Formal\n",
        "import dadmatools.pipeline.language as language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R6oG_mImJF8",
        "outputId": "ead2f5cd-7535-4239-ab55-da7e1167ecfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file cache/dadmatools/fa_tokenizer.pt: : 639kB [00:01, 552kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NORMALIZER"
      ],
      "metadata": {
        "id": "32pR7JQbmp1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer(\n",
        "    full_cleaning=False,\n",
        "    unify_chars=True,\n",
        "    refine_punc_spacing=True,\n",
        "    remove_extra_space=True,\n",
        "    remove_puncs=True,\n",
        "    remove_html=True,\n",
        "    remove_stop_word=True,\n",
        "    replace_email_with=\"<EMAIL>\",\n",
        "    replace_number_with=\"<NUMBER>\",\n",
        "    replace_url_with=\"<URL>\",\n",
        "    replace_mobile_number_with=\"<MOBILE>\",\n",
        "    replace_emoji_with=\"<EMOJI>\",\n",
        "    replace_home_number_with=\"<HOME>\"\n",
        ")\n",
        "\n",
        "sample = \"\"\"\n",
        "<div>\n",
        "مقاله‌ای که در سال ۲۰۲۱ منتشر شد، نشان داد که ۴۲٪ از کاربران از ایمیل‌های ناشناس استفاده می‌کنند.\n",
        "برای اطلاعات بیشتر به سایت ما مراجعه کنید: https://www.example.com\n",
        "می‌توانید از طریق ایمیل contact@example.com با ما تماس بگیرید.\n",
        "شماره تلفن همراه: ۰۹۱۲۳۴۵۶۷۸۹\n",
        "شماره تلفن منزل: ۰۲۱۸۷۶۵۴۳۲۱\n",
        "ما در شبکه‌های اجتماعی نیز فعال هستیم. 😊\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "print('input:', sample)\n",
        "print('normalized input:', normalizer.normalize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k5I9hMhlH5P",
        "outputId": "020781bc-d52a-40a3-db97-7c2ba70180b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: \n",
            "<div>\n",
            "مقاله‌ای که در سال ۲۰۲۱ منتشر شد، نشان داد که ۴۲٪ از کاربران از ایمیل‌های ناشناس استفاده می‌کنند.\n",
            "برای اطلاعات بیشتر به سایت ما مراجعه کنید: https://www.example.com\n",
            "می‌توانید از طریق ایمیل contact@example.com با ما تماس بگیرید.\n",
            "شماره تلفن همراه: ۰۹۱۲۳۴۵۶۷۸۹\n",
            "شماره تلفن منزل: ۰۲۱۸۷۶۵۴۳۲۱\n",
            "ما در شبکه‌های اجتماعی نیز فعال هستیم. 😊\n",
            "</div>\n",
            "\n",
            "normalized input: مقاله‌ای سال <NUMBER> منتشر <NUMBER>٪ کاربران ایمیل‌های ناشناس اطلاعات سایت مراجعه <URL> می‌توانید ایمیل <EMAIL> تماس شماره تلفن همراه <MOBILE> شماره تلفن منزل <HOME><NUMBER> شبکه‌های اجتماعی فعال <EMOJI>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFORMAL2FORMAL"
      ],
      "metadata": {
        "id": "J-_UGVf5ohKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formalizer = Informal2Formal()\n",
        "\n",
        "informal_sample = 'این یه جمله غیررسمیه که اگه خواستین میتونین به رسمی تبدیلش کنین'\n",
        "\n",
        "print('input:', informal_sample)\n",
        "print('formalized input:', formalizer.translate(informal_sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaJ99ptJnzR_",
        "outputId": "1588ce3e-0175-4d2d-9564-e226d330b6e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3gram.bin: 2.30GB [01:36, 25.7MB/s]\n",
            "assets.pkl: 3.14MB [00:00, 17.3MB/s]\n",
            "irregular_verb_mapper.csv: 100%|██████████| 1.57k/1.57k [00:00<00:00, 4.14MB/s]\n",
            "verbs.csv: 100%|██████████| 39.4k/39.4k [00:00<00:00, 7.81MB/s]\n",
            "Model fa_tokenizer exists in cache/dadmatools/fa_tokenizer.pt\n",
            "input: این یه جمله غیررسمیه که اگه خواستین میتونین به رسمی تبدیلش کنین\n",
            "formalized input:  این یک جمله غیررسمی است که اگر خواستید می‌توانید به رسمی تبدیلش بکنید\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup NLP pipeline with various modules"
      ],
      "metadata": {
        "id": "lYQ_bbwGqJtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pips = 'ner,sent,pos,spellchecker,dep,kasreh,itf,lem'\n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "# Example text for NLP pipeline\n",
        "nlp_text = \"من در دانشگاه علم و صنعت درس می‌خوانم. این یک جمله تستی است.\"\n",
        "\n",
        "# Apply the NLP pipeline to the text\n",
        "doc = nlp(nlp_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwRu4fvvpGHI",
        "outputId": "9e9fc3d6-5c06-479b-d343-6d12f93e3a15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained XLM-Roberta, this may take a while...\n",
            "Model fa_tokenizer exists in cache/dadmatools/fa_tokenizer.pt\n",
            "Loading tokenizer for persian\n",
            "Loading tagger for persian\n",
            "Loading multi-word expander for persian\n",
            "Loading lemmatizer for persian\n",
            "Loading NER tagger for persian\n",
            "Loading Kasreh tagger for persian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file cache/dadmatools/3gram.bin already exist\n",
            "Model fa_tokenizer exists in cache/dadmatools/fa_tokenizer.pt\n",
            "==================================================\n",
            "Active language: persian\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  1.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. SpellChecker Module"
      ],
      "metadata": {
        "id": "Yz3Ic1omsBUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'نمازگذاران به پا خواستند.'\n",
        "doc = nlp(sample)\n",
        "doc['spellchecker']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTZ__PYtqfZg",
        "outputId": "398e2318-0c6a-40f3-b641-23a71ee253d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'orginal': 'نمازگذاران به پا خواستند.',\n",
              " 'corrected': 'نمازگذاران به پا خواستند.',\n",
              " 'checked_words': []}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you know, the corrected sentence should be: <br>\n",
        "نمازگزاران به پاخاستند. <br>\n",
        "while spellchecker module returned:<br>\n",
        "نمازگذاران به پا خواستند."
      ],
      "metadata": {
        "id": "pX1BYiMstvlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. LEM Module\n"
      ],
      "metadata": {
        "id": "ZsN2-R4eufFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'پرستو در آسمان پراوز می‌کرد'\n",
        "doc = nlp(sample)\n",
        "for sentence in doc['sentences']:\n",
        "  for token in sentence['tokens']:\n",
        "    print(token['lemma'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QRsMVFNtqIr",
        "outputId": "a7572881-ec3c-4d67-e931-76d141a43788"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  1.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پرست#پرس\n",
            "در\n",
            "آسمان\n",
            "پراوزت#پراو\n",
            "کرد#کن\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you can see the module has lemmatized word پرستو which is incorrect."
      ],
      "metadata": {
        "id": "dN3krU50vQIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. NER Module"
      ],
      "metadata": {
        "id": "Hn7jkgSzvfee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'دوستان فیلم خوب هالیوودی چی پیشنهاد می‌دین؟ ژانرش مهم نیست.'\n",
        "doc = nlp(sample)\n",
        "for sentence in doc['sentences']:\n",
        "  for token in sentence['tokens']:\n",
        "        print(f\"token:{token['text']}     ner:{token['ner']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQXBUy3lvEjQ",
        "outputId": "63ae2e29-8e3c-4748-f06e-71e1c50cb010"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  2.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token:دوستان     ner:O\n",
            "token:فیلم     ner:O\n",
            "token:خوب     ner:O\n",
            "token:هالیوودی     ner:O\n",
            "token:چی     ner:O\n",
            "token:پیشنهاد     ner:O\n",
            "token:می‌دین     ner:O\n",
            "token:؟     ner:O\n",
            "token:ژانرش     ner:O\n",
            "token:مهم     ner:O\n",
            "token:نیست     ner:O\n",
            "token:.     ner:O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you can see, ner module has returned 0 for every single token. not the true ner lables."
      ],
      "metadata": {
        "id": "-r2xph4VwUpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. POS Module"
      ],
      "metadata": {
        "id": "v9iqA7GVwnAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'یکی از تلخ‌ترین سرگذشت‌هایی بود که شنیدم.'\n",
        "doc = nlp(sample)\n",
        "for sentence in doc['sentences']:\n",
        "  for token in sentence['tokens']:\n",
        "        print(f\"token:{token['text']}     upos:{token['upos']}     xpos:{token['xpos']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U02ThMZ5wG0b",
        "outputId": "dcb63fa8-4f07-4cf9-bed2-141c962abd0c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token:یکی     upos:NUM     xpos:NUM\n",
            "token:از     upos:AUX     xpos:V_PRS\n",
            "token:تلخ‌ترین     upos:ADJ     xpos:ADJ_SUP\n",
            "token:سرگذشت‌هایی     upos:NOUN     xpos:N_PL\n",
            "token:بود     upos:VERB     xpos:V_PA\n",
            "token:که     upos:SCONJ     xpos:CON\n",
            "token:شنیدم     upos:VERB     xpos:V_PA\n",
            "token:.     upos:PUNCT     xpos:DELM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the word سرگذشت should have object tag."
      ],
      "metadata": {
        "id": "GlanzKa9xekK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. ITF Module"
      ],
      "metadata": {
        "id": "kGbrQBUnxqxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'اگر گرفتیش به منم قرضش بده'\n",
        "doc = nlp(sample)\n",
        "doc['itf']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JX15bveVxWc8",
        "outputId": "fc6f7eea-5463-47d1-a838-322dbc64c063"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  3.63it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' اگر گرفتیش به منم قرضش بده'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you can see, the returned sentence is not in formal form."
      ],
      "metadata": {
        "id": "WCbdUVu3yMVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. DEP Module"
      ],
      "metadata": {
        "id": "P5du0Vccyeph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'مهلا کتاب به دست به سمت کتابخانه حرکت کرد.'\n",
        "doc = nlp(sample)\n",
        "print(f\"Text    Head    Deprel\")\n",
        "for sentence in doc['sentences']:\n",
        "  dependency_parsing = sentence['tokens']\n",
        "  for token in dependency_parsing:\n",
        "    print(token['text'],'\\t',token['head'],'\\t',token['deprel'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZYsPIu_xx40",
        "outputId": "ec626b47-f6a5-4061-dd26-f581199bd785"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text    Head    Deprel\n",
            "مهلا \t 9 \t nsubj\n",
            "کتاب \t 1 \t fixed\n",
            "به \t 2 \t fixed\n",
            "دست \t 1 \t fixed\n",
            "به \t 7 \t case\n",
            "سمت \t 5 \t fixed\n",
            "کتابخانه \t 9 \t obl\n",
            "حرکت \t 5 \t compound:lvc\n",
            "کرد \t 0 \t root\n",
            ". \t 3 \t punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the adverb کتاب به دست is not correctly recognized."
      ],
      "metadata": {
        "id": "Tp_uAGZozvim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. TOK Module"
      ],
      "metadata": {
        "id": "LeK_IS1H0C-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'پارسال تابستان به واشنگتن دی سی رفته بودم'\n",
        "doc = nlp(sample)\n",
        "for sentence in doc['sentences']:\n",
        "  for token in sentence['tokens']:\n",
        "    print(token['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRIvVxB4ytcU",
        "outputId": "f6ada3de-8a8e-4845-a778-c2a33696ef3d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پارسال\n",
            "تابستان\n",
            "به\n",
            "واشنگتن\n",
            "دی\n",
            "سی\n",
            "رفته\n",
            "بودم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the word واشنگتن دی سی should be a single token."
      ],
      "metadata": {
        "id": "o-rJhCzE0Q-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. SENT Module"
      ],
      "metadata": {
        "id": "LdXxrM1a0bAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'برخلاف ایرادات متعددی که در ساخت فیلم وجود داشت توانست نظر منتقدین را جلب کند'\n",
        "doc = nlp(sample)\n",
        "doc['sentiment']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgEq7xun0Pjz",
        "outputId": "44f59b6f-b163-4e7e-ca4f-a8a62cb98a5e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'negative', 'score': 0.388910710811615}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentence does not contain negative sentiment."
      ],
      "metadata": {
        "id": "z6JZwv0x1BHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. KASREH Module"
      ],
      "metadata": {
        "id": "2gIpw8ty1Ka0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample =  'کتاب تخصصی زیست شناسی مریم را قرض گرفتم.'\n",
        "doc = nlp(sample)\n",
        "for sentence in doc['sentences']:\n",
        "  for token in sentence['tokens']:\n",
        "        print(f\"{token['text']}     kasreh:{token['kasreh']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Iwfu4wc0-_e",
        "outputId": "86f66b93-d025-4188-d31a-3c0997f1904e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کتاب     kasreh:S-kasreh\n",
            "تخصصی     kasreh:S-kasreh\n",
            "زیست     kasreh:S-kasreh\n",
            "شناسی     kasreh:S-kasreh\n",
            "مریم     kasreh:O\n",
            "را     kasreh:O\n",
            "قرض     kasreh:S-kasreh\n",
            "گرفتم     kasreh:O\n",
            ".     kasreh:O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the word قرض doesn't contain kasreh."
      ],
      "metadata": {
        "id": "1Q2uWfZq1lzc"
      }
    }
  ]
}